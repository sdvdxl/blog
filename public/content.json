{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-11-04T08:30:45.794Z","updated":"2017-11-04T08:30:45.794Z","comments":true,"path":"baidu_verify_ym0qdqCEoR.html","permalink":"http://yoursite.com/baidu_verify_ym0qdqCEoR.html","excerpt":"","text":"ym0qdqCEoR"},{"title":"自动测试工具Gauge","date":"2017-07-31T06:49:23.000Z","updated":"2017-11-04T04:50:10.200Z","comments":true,"path":"gauge.html","permalink":"http://yoursite.com/gauge.html","excerpt":"","text":"Gauge 是一款轻量级、跨平台自动化测试工具集。规则文件语法可以使用markdown语法编写。另外还可以使用你喜欢的语言来编写业务规则代码比如 go、 java、ruby等语言。Gauge还提供了输出插件，可以将执行结果导出为html或者xml，甚至flash，方便查看。 Quick Start有小伙伴不喜欢翻译官方文档的内容，让我开门见山，直接实战。那我们直接按照步骤来创建一个 JAVA 版的测试项目。前提是已经安装好了 gauge ，html-report 和 JAVA 插件，如果需要安装帮助，请参考下面的安装步骤或者直接参考官网的安装文档。 创建项目命令行执行 gauge init java，执行成功后悔看到下面的日志输出: 1234Downloading java.zip...Copying Gauge template java to current directory ...Successfully initialized the project. Run specifications with &quot;gauge specs/&quot;. 这样就创建了一个基于 JAVA 语言的测试项目，并且提供了demo 示例，切换到项目目录下，直接执行 gauge run specs 进行测试。会有以下输出: 123456789# Specification Heading ## Vowel counts in single word ✔ ✔ ## Vowel counts in multiple word ✔ ✔Successfully generated html-report to =&gt; /Users/du/Desktop/test/reports/html-reportSpecifications: 1 executed 1 passed 0 failed 0 skippedScenarios: 2 executed 2 passed 0 failed 0 skippedTotal time taken: 93ms 测试已经完成，并且会在项目目录下 reports/html-report 生成测试报告，直接用浏览器打开 index 即可查看测试报告: 这是报告的首页，上面标注了总共多少个 spec 文件，失败个数，成功个数，跳过多少个。并且可以通过左下角的搜索或者直接点标题来查看对应的测试文件或者用例。 每个spce 的测试报告都会将测试的用例结果打印出来并且用不同颜色表示出来，绿色则是成功，红色则是失败。其中一个大的圆点表示的是一个测试场景，圆点下每个绿色线条代表一个具体的测试用例。 下面我们来尝试修改一下 The word &quot;gauge&quot; has &quot;3&quot; vowels 这个用例。打开 specs/example.spec 这个文件，在第18行我们就会找到上面提到的这个测试用例，然后打开 src/test/java/StepImplementation.java 这个文件，第21行就是这个测试用例的实现代码。 可以看到，这个测试用例有两个参数，word 和 expectedCount，现在我们将 specs/example.spec 中的这个测试用例的参数 “expectedCount” 改为 1，然后重新运行 gauge run specs ，执行过程中控制台抛出了异常信息，接收后，打开 生成的 html 文件查看，有没有通过的测试用例: 尝试增加一个场景和一个用例:在 specs/example.spec 中最后追加以下代码: 1234测试场景---------------* 新增的一个测试\"成功\" 然后在 StepImplementation.java 中增加下面的方法: 1234@Step(\"新增的一个测试&lt;param&gt;\")public void testParam(String param) &#123; assertEquals(\"成功\",param);&#125; 运行 gauge run specs，执行结束后打开报告，我们新定义的场景和用例豆子行成功了；现在再修改一下 sample.spce文件，在最后再追加以下代码: 1新增的一个测试\"失败\" 运行 gauge run specs，执行结束后打开报告，我们新定义的场景和用例豆子行失败了，因为参数值和预期不一致，我们的代码抛出了异常，则代表该测试用例是失败的。 细心的同学可能发现，每个场景开头都有 Vowels in English language are &quot;aeiou&quot;. 这个测试用例，这是因为这个测试用例是在 spec 层级下，会在执行过程中追加到每个场景的开始。另外会发现有切换窗口的动作，并且在失败的用例中有图片生成，这个则是默认配置，失败则截取屏幕图片的配置造成的，如果不想截图，则可以修改 env/default/default.properties 文件第13行的 screenshot_on_failure 属性为 false 即可。 至此，我们学会了如何运行测试用例，如何编写测试用例，如果需要其他知识点，比如数据到不同范围，如何在用例失败的情况下继续执行该场景的其他用例，请参考下面的介绍和官网知识。 主要特点 A rich markup based on markdown Simple, Flexible and Rich Syntax Business Language Tests : Supports the concept of executable documentation. Consistent Cross Platform/Language Support for writing test code. Currently supported languages. Open Source, so it could be shared freely and improved by others as well. A modular architecture with Plugins support. Extensible through Plugins and Hackable. Supports External Data Sources. Helps you create Maintainable and Understandable test suites.IDE Support. 概念(术语)Specifications (spec)一个spec就是一个spec文件，用来定义规则。使用markdown的一级标题来定义该文件的说明 ，比如:12Specification name================== 或者1# Specification name Scenarios一个 scenarios 可以认为是一个组，定义在spec文件中。使用markdown的二级标题定义12Scenarios---------------- 或者1## Scenarios Tags可以给 Spec 文件和 Scenarios 打标签。 12345678910Specification name================tags: s1,test1Scenarios----------------tags: s1, s2 Steps这个就是具体的测试用例。使用markdown的 * 声明一个用例。 1* Step Name step 还支持参数，包括静态参数，动态参数，表格参数 ，特殊参数 静态参数静态参数用双引号包裹:1* Check &quot;product&quot; exists 动态参数动态参数使用尖括号包裹，跟下面的特殊参数结合使用 1* Check &lt;product&gt; exists 表格参数表格参数，就是一个二维表格所构成的数据。 12345* Step that takes a table | id | name | |-----|---------| | 123 | John | | 456 | Mcclain | 注意，定义和表格之间没有空行，表格是直接跟在定义的语句下面 特殊参数1&lt;prefix:value&gt; prefix 支持 file 和 table 12345// file * Check if &lt;file:/work/content.txt&gt; is visible// table* Check if the users exist &lt;table:/Users/john/work/users.csv&gt; 注释没有特殊的语法，任何一般的文本都可以作为注释 其他支持图片，连接 安装首先要安装Gauge程序，可以从这里https://getgauge.io/get-started.html下载对应平台的安装程序。Windows平台没有测试。 Macmac用户可以使用brew安装，brew update &amp;&amp; brew install gauge，也可以从上面的地址下载安装包，然后执行安装即可。 Linuxlinux用户需要下载linux程序，下载下来是个安装包，假设我们放到 /opt/gauge/gauge-0.9.0-linux.x86_64.zip (最好创建一个目录，因为解压后不会生成单独的目录)。切换到 /opt/gauge 执行 unzip gauge-0.9.0-linux.x86_64.zip，得到以下文件：12345├── bin│ ├── gauge│ └── gauge_screenshot└── install.sh 执行 ./install.sh ，一直按回车，使用默认配置即可安装完成。 其他环境如果要使用java，则需要安装jdk，如果要使用C#，则需要安装.net环境，总之，需要什么语言，就需要安装什么环境。 安装完成后，命令行输入gauge，如果出现 gauge 的帮助信息，则说明安装成功，否则请按照官方文档进行安装。 插件Gauge 支持一些插件，比如生成java和执行代码的插件，生成html报告的插件等等，插件支持: java ruby flash go js html-report xml-report python spectacle(可能还有其他的，暂时没有去搜索。) 插件安装方式: gauge install &lt;plugin-name&gt;，如果下载太慢，可以手动下载，然后使用本地安装方式安装 gauge install --file &lt;path-plugin&gt; 实战这里我们使用的是JAVA方式。所以这里需要安装JAVA插件 gauge install java，并且我们要以html方式展示报告，需要安装 html report 插件 gauge install html-report 执行 gauge init java 会生成一个java的gauge项目:123456789101112├── env│ └── default│ ├── default.properties│ └── java.properties├── libs├── manifest.json├── specs│ └── example.spec└── src └── test └── java └── StepImplementation.java 想要执行测试，执行命令 gauge run specs 即可， 最后会输出执行结果，同时会在项目下生成reports/html-report html报告，用浏览器打开即可查看结果。 结构说明 目录 说明 env 环境变量，可以配置不通参数，适用于不通环境，比如测试环境还是线上环境 libs 其他要依赖的java类库要放到这里 manifest.json 项目配置文件 spces 测试用例描述文件存放的地方 src java 代码放置地方 命令说明上面我们使用了命令 gauge run spces 来运行测试用例，run 后面跟的 spces就是项目中的 specs 目录，代表执行这个目录下的所有文件，也可以单独执行一个文件执行，比如 gauge run specs/example.spec。 文件编写用例定义在spec文件中，具体实现的代码则是在java代码中实现的。 进阶"},{"title":"","date":"2017-11-04T08:30:45.793Z","updated":"2017-11-04T08:30:45.793Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"强调 水电费水电费 1There is a literal backtick (`) here."},{"title":"Categories","date":"2017-11-04T08:30:45.794Z","updated":"2017-11-04T08:30:45.794Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-11-04T08:30:45.864Z","updated":"2017-11-04T08:30:45.864Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2017-11-04T04:54:40.564Z","updated":"2017-11-04T04:54:40.564Z","comments":true,"path":"2017/11/04/hello-world/","link":"","permalink":"http://yoursite.com/2017/11/04/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"RESTFul API 错误的设计","slug":"other/RESTFul-API-错误的设计","date":"2017-05-13T15:15:53.000Z","updated":"2017-05-22T02:44:26.000Z","comments":true,"path":"2017/05/13/other/RESTFul-API-错误的设计/","link":"","permalink":"http://yoursite.com/2017/05/13/other/RESTFul-API-错误的设计/","excerpt":"","text":"不管是WEB还是移动端，如果需要调用Http接口，那么不可避免要处理各种错误，包括客户端参数完整性校验，类型校验，系统错误等。如果一个设计规范的接口错误返回值，不但可以规范调用方统一处理方式，给出更合理的提示，而且后端也有着接口返回规范的作用。这里讨论的其实并不单纯是返回的错误设计，也包括业务上的错误设计。 接口返回设计接口返回需要有http status，错误说明，并且要提供一个完整的错误列表，可以通过简单的错误说明或者错误代码查阅到详细的错误原因。简要错误说明可以只包含简短的文字描述，可以包含错误代码。如果说哪种方式更为合理，我更倾向于错误代码+简要错误说明。如果只有简要错误说明，这个错误描述可能有些描述不合理，如果后端改了说明，那么对应的错误映射也要更改，调用方也要跟着修改。如果只有错误代码，调用方难以一眼看出错误原因，需要查阅错误码，这个比较啰嗦。如果同时提供错误码和简要错误说明，调用方可以通过简要说明快速定位问题，如果要处理错误逻辑，根据错误码表映射即可，这样即使描述改变了，但是错误码是永久不变的，不会影响调用方的处理逻辑。返回的结构如下：123456HTTP/1.1 200 OKContent-Type: application/json;charset=UTF-8&#123; \"msg\": \"错误说明\"&#125; 服务端错误设计和\b处理根据RESTful接口的http status，对于错误，可以分为以下几大类（常用）： 权限错误 401 参数错误 400 重复，冲突 409 找不到资源 404 服务器错误 500 对于以上5中分类，除了第5种系统服务器错误外，其余四种我们可以分别设计一个错误（异常）类型。至于第5中服务器错误\b为什么不需要，因为我们需要根据上面四种错误\b是需要单独判断的，如果不在上面的类型中，直接返回500即可。对于上面的错误，在返回或者抛出的时候，应该具体到原因，比如： 参数错误，要写明哪里错误，是手机号不合法还是邮箱不合法，还是哪个参数类型不对，这样便于调用方定位问题。 以Java为例，我们可以定义以下异常：12345678910111213141516public class AuthExcepption extends RuntimeException &#123; protected int code; public AuthExcepption(int code, String msg) &#123; this.code = code; super(msg); &#125; public int getCode() &#123; return code; &#125;&#125;// 对于其他三个也是这个格式。// IllegalArgumentException// DuplicatedException// NotFoundException 在做异常统一处理的时候，可以判断极少的类型即可，下面是 spring mvc 的统一异常处理方式：12345678910111213141516171819202122232425262728293031323334class Body &#123; private int code; private String msg; // setter and getter public Body(int code, String msg) &#123; this.code = code; this.msg = msg; &#125;&#125;@ExceptionHandler(value = &#123;AuthExcepption.class&#125;)public ResponseEntity&lt;Object&gt; handleAuthExcepption(AuthExcepption ex, WebRequest request) &#123; // 记录日志 return new ResponseEntity(new Body(ex.getCode(),ex.getMessage()),HttpStatus.UNAUTHORIZED);&#125;@ExceptionHandler(value = &#123;IllegalArgumentException.class&#125;)public ResponseEntity&lt;Object&gt; handleIllegalArgumentException(IllegalArgumentException ex, WebRequest request) &#123; // 记录日志 return new ResponseEntity(new Body(ex.getCode(),ex.getMessage()), HttpStatus.BAD_REQUEST);&#125;@ExceptionHandler(value = &#123;DuplicatedException.class&#125;)public ResponseEntity&lt;Object&gt; handleIllegalArgumentException(DuplicatedException ex, WebRequest request) &#123; // 记录日志 return new ResponseEntity(new Body(ex.getCode(),ex.getMessage()), HttpStatus.CONFLICT);&#125;@ExceptionHandler(value = &#123;NotFoundException.class&#125;)public ResponseEntity&lt;Object&gt; handleIllegalArgumentException(NotFoundException ex, WebRequest request) &#123; // 记录日志 return new ResponseEntity(new Body(ex.getCode(),ex.getMessage()),HttpStatus.NOT_FOUND);&#125; 在业务层那么需要这么处理，抛出一种异常，附带具体的错误码和简要消息：12345public void addUser(User user) &#123; if (!isValid(user.getPhone())) &#123; throw new IllegalArgumentException(10001,\"手机：\"+user.getPhone()+\" 不合法\"); &#125;&#125; 在上面的业务处理示例中，我们采用了传递错误码的形式，另一种处理方式是定义一个该错误码对应的异常，比如：12345public class PhoneIllegalException extends IllegalArgumentException&#123; public PhoneIllegalException (String msg) &#123; super(10001,msg); &#125;&#125; 然后业务层就可以直接抛出此异常了：12345public void addUser(User user) &#123; if (!isValid(user.getPhone())) &#123; throw new PhoneIllegalException(\"手机：\"+user.getPhone()+\" 不合法\"); &#125;&#125;","categories":[],"tags":[{"name":"API","slug":"API","permalink":"http://yoursite.com/tags/API/"},{"name":"RESTFul","slug":"RESTFul","permalink":"http://yoursite.com/tags/RESTFul/"},{"name":"错误","slug":"错误","permalink":"http://yoursite.com/tags/错误/"},{"name":"异常","slug":"异常","permalink":"http://yoursite.com/tags/异常/"}]},{"title":"安装Hbase分布式集群","slug":"hadoop/安装hbase分布式集群","date":"2017-02-15T09:44:43.000Z","updated":"2017-05-14T08:10:16.000Z","comments":true,"path":"2017/02/15/hadoop/安装hbase分布式集群/","link":"","permalink":"http://yoursite.com/2017/02/15/hadoop/安装hbase分布式集群/","excerpt":"","text":"以下操作都是在hadoop这个用户下 下载最新版hbase，放到/home/hadoop目录下，解压，生成目录 hbase-1.2.4 下载zookeeper，放到 /home/hadoop/zookeeper-3.4.9， 解压生成 zookeeper-3.4.9 目录 编辑conf下hbase-site.xml123456789101112131415161718 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave1,slave2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/hadoop/zookeeper-3.4.9&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 其中 hbase.zookeeper.quorum 配置是hbase集群机器的名字，上面的值代表在master，slave1，和slave2上启动hbase和zookeeperhbase.zookeeper.property.dataDir 是用于配置zookeeper安装目录的，这里我把zookeeper安装到了 /home/hadoop/zookeeper-3.4.9 修改 conf下 hbase-env.sh修改JAVA_HOME，将其指向具体的java安装目录，（最小版本要求是1.7） 启动hbase 1bin/start-hbase.sh 测试输入 bin/hbase shell 进入交互环境， 创建表 create &#39;t1&#39;,{NAME =&gt; &#39;f1&#39;, VERSIONS =&gt; 2},{NAME =&gt; &#39;f2&#39;, VERSIONS =&gt; 2} 列出表 list如果没有出错那么安装成功。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"域名说明","slug":"域名说明","date":"2017-02-15T03:02:30.000Z","updated":"2017-05-22T02:46:06.000Z","comments":true,"path":"2017/02/15/域名说明/","link":"","permalink":"http://yoursite.com/2017/02/15/域名说明/","excerpt":"","text":"由于该域名尚未备案，导致不能正常访问文章，请切换到域名 http://du.uke.wiki访问。","categories":[],"tags":[]},{"title":"Gnome-Shell插件","slug":"linux/Gnome-Shell插件","date":"2016-08-02T08:24:18.000Z","updated":"2017-11-04T08:30:45.785Z","comments":true,"path":"2016/08/02/linux/Gnome-Shell插件/","link":"","permalink":"http://yoursite.com/2016/08/02/linux/Gnome-Shell插件/","excerpt":"","text":"状态栏菜单按钮 Dock 状态栏音乐控制 菜单列表 Tab 切换 任务栏 状态栏数字键+大写键状态 番茄工作法插件 系统托盘 状态栏网速 状态栏日历","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"gnome","slug":"gnome","permalink":"http://yoursite.com/tags/gnome/"}]},{"title":"SpringXD HA 配置","slug":"spring/SpringXD HA 配置","date":"2016-06-29T08:34:48.000Z","updated":"2017-11-04T08:30:45.789Z","comments":true,"path":"2016/06/29/spring/SpringXD HA 配置/","link":"","permalink":"http://yoursite.com/2016/06/29/spring/SpringXD HA 配置/","excerpt":"","text":"SpringXD官方文档 上说的不是很清楚，而且有些配置（如 配置 hadoop namenode ha ）并没有在上面说明，只是简单的说明了怎么配置 namenode ，如果没有ha配置，那么在生产环境中会令人头痛。 XD Admin HA说明在 官方文档 中，有说如何配置，就是通过启动多个admin ，然后通过 zookeeper 管理。Spring XD 要求只有一个主节点来和 Container 交互，例如 Stream 发布等。同时，这些操作都是按顺序处理的。假如只有一个 admin ，那么就存在单点失败的风险，因此，在生产环境中推荐做法是启动 2 个或者多 admin 。注意：在有多个 admin 节点的时候，每个 admin 都可以处理 REST 请求，但是只有一个实例会作为 Leader 处理请求并更新运行时的状态。如果 Leader 宕掉，另一个可用的admin就会成为新的 Leader 来接管任务。当然，Spring XD 的HA不只是他自身要求ha，还需要依赖外部服务，如：zookeeper，MessageBus 等 HA 配置。 配置信息如果要配置 admin 的 ha，那么启动多个 admin 即可，但是请注意，如果是在同一台机器上部署多个admin，需要在启动时候添加如下参数以防止和默认的端口（9393）冲突：--httpPort 用来指定rest api端口--mgmtPort 用来指定管理端口如果在不同机器上启动，只需配置相同的配置文件，然后启动即可。 XD Container HA当添加 Container 的时候，Spring XD 可以动态水平扩展，也就是说不需要额外什么操作，只需像第一次启动 Container 一样输入命令 bin/xd-container 启动即可。 XD Hadoop namenode HA如果 xd 中创建 stream 或者其他任务是用到了 hdfs 的功能，那么要配置 hadoop 的namenode ，要在 xd/config/servers.xml中的 spring.hadoop.fsUri 的配置项中配置。注意，这个地方只允许配置一个 host，如果有备用 namenode ，是不允许配置在这个地方的。但是这样配置是有问题的，就是存在 hadoop 的 namenode 主从切换后 xd 的 stream 无法写入 hdfs 或者读取 hdfs 的故障。要解决这个问题，我们要再 xd/config/hadoop.properties 中配置如下配置项：12345dfs.nameservices=MyClusterdfs.ha.namenodes.MyCluster=nn1,nn2dfs.namenode.rpc-address.MyCluster.nn2=hadoop-master1-host:8020dfs.namenode.rpc-address.MyCluster.nn1=hadoop-master2-host:8020dfs.client.failover.proxy.provider.MyCluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider 其中，配置项中所有的 MyCluster 要换成自己项目中 hadoop 集群的名字，然后在 xd/config/servers.xml 中 spring.hadoop.fsUri 值配置成 hdfs://MyCluster:8020（注意8020端口换成自己配置的），hadoop-master1-host 和 hadoop-master1-host 换成自己集群的 hadoop的 master 的主机名字或者ip。这样配置后，重新启动 admin 和 container 就会自动检测 hadoop 的 namenode 主从，并自行切换。如果要在 xd-shell中使用，需要登录shell之后（如果有安全设置，还需要先用密码登录成功），输入一下命令：123456hadoop config props set --property dfs.nameservices=MyClusterhadoop config props set --property dfs.ha.namenodes.MyCluster=nn1,nn2hadoop config props set --property dfs.namenode.rpc-address.MyCluster.nn1=hadoop-master1-host:8020hadoop config props set --property dfs.namenode.rpc-address.MyCluster.nn2=hadoop-master2-host:8020hadoop config props set --property dfs.client.failover.proxy.provider.MyCluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProviderhadoop config fs --namenode hdfs://MyCluster MyCluster，hadoop-master1-host 和 hadoop-master2-host 同上配置。对于 shell 来说，单纯配置 hadoop 的主 namenode也是可以的，因为这个配置只是对 shell 起作用。如果觉得每次打开shell都要输入上面几行配置太繁琐的话，可以将 xd/config/hadoop.properties 中配置的项目添加一份到shell/config/hadoop.properties 即可，这样在shell中操作hdfs，只需配置 hadoop config fs --namenode hdfs://MyCluster:8020 即可。 附言对于如何在xd中使用hadoop的namenode ha配置，xd 官方文档中并未见说明，而是百般Goole之后得到的结果，而且由于xd资料尚少，搜索结果不佳，最后通过搜索关键词 xd namenode fail 才找到解决方案，请参见这里。","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"xd","slug":"xd","permalink":"http://yoursite.com/tags/xd/"}]},{"title":"sbt源配置","slug":"spark/sbt源配置","date":"2016-06-20T03:21:13.000Z","updated":"2017-11-04T08:30:45.788Z","comments":true,"path":"2016/06/20/spark/sbt源配置/","link":"","permalink":"http://yoursite.com/2016/06/20/spark/sbt源配置/","excerpt":"","text":"sbt 默认源下载有点慢，我们可以调教它，让它从我们自己配置的源下载。配置源在 .sbt （默认是在用户名下）文件夹中创建 repositories 文件，然后添加如下内容:1234567891011121314151617181920212223[repositories] local my: http://o8r69qphn.bkt.clouddn.com/ Nexus osc: http://maven.oschina.net/content/groups/public/ Nexus osc thirdparty: http://maven.oschina.net/content/repositories/thirdparty/ central: http://central.maven.org/maven2/[ivy] local my: http://o8r69qphn.bkt.clouddn.com/ Nexus osc: http://maven.oschina.net/content/groups/public/ Nexus osc thirdparty: http://maven.oschina.net/content/repositories/thirdparty/ central: http://central.maven.org/maven2/ TypeSafe: https://oss.sonatype.org/content/repositories/releases/ #proxy库 typesafe-ivy-releases: http://dl.bintray.com/typesafe/ivy-releases/ typesafe-maven-releases: http://dl.bintray.com/typesafe/maven-releases/ typesafe-sbt-plugin-releases: http://dl.bintray.com/sbt/sbt-plugin-releases/ #group库 ivy-releases : typesafe-ivy-releases,typesafe-sbt-plugin-releases 上面 repositories 是说加载maven镜像中的库文件从这个标签下的路径中找， local 代表从本地中找，默认是 .M2 中，下面的都是自定义源，名字随便取。下面 ivy 是加载ivy库的。 修改加载配置项单纯修改上面的源还不足以让sbt加载我们的源。打开 sbt 软件安装位置下的 conf/sbtopts 文件，在其中添加：1-Dsbt.override.build.repos=true 然后就可以生效了。 没有深入研究，如果有错还请指出","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"sbt","slug":"sbt","permalink":"http://yoursite.com/tags/sbt/"}]},{"title":"Linux shell 命令","slug":"linux/Linux-Shell-命令","date":"2016-05-25T01:34:07.000Z","updated":"2017-11-04T08:30:45.785Z","comments":true,"path":"2016/05/25/linux/Linux-Shell-命令/","link":"","permalink":"http://yoursite.com/2016/05/25/linux/Linux-Shell-命令/","excerpt":"","text":"lsof适用于ip4 1lsof -Pnl +M -i4 | grep port 适用于ip6 1lsof -Pnl +M -i6 | grep port awk杀掉名字一样的java进程1jps |grep SparkSubmit | awk &apos;&#123;print &quot;kill -9 &quot; $1&#125;&apos; | sh 如果仅仅是打印命令，则后面的管道和sh不需要加，如下1jps |grep SparkSubmit | awk &apos;&#123;print &quot;kill -9 &quot; $1&#125;&apos; find删除找到的符合条件的文件（或者目录）1find . -iname target -exec rm -rf &#123;&#125; \\;","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"sbt-idea-入门及配置","slug":"spark/sbt-idea-入门及配置","date":"2016-05-10T01:35:48.000Z","updated":"2017-11-04T08:30:45.788Z","comments":true,"path":"2016/05/10/spark/sbt-idea-入门及配置/","link":"","permalink":"http://yoursite.com/2016/05/10/spark/sbt-idea-入门及配置/","excerpt":"","text":"Java 环境配置这个就不多说了，这是前提条件，请自行安装后配置正确，如果不清楚请自行搜索 java 环境变量配置 相关问题。 Scala 配置首先要要配置 scala 环境。从官方地址下载，这里我们使用scala2.10.6版本，所以从这里下载对应的平台版本。Windows请下载 scala-2.10.6.zip ，MacOS和Linux请下载 scala-2.10.6.tgz 。 下载完成后，解压到一个目录，然后配置环境变量 SCALA_HOME ，把scala的解压后的绝对路径配置到 SCALA_HOME ，然后增加 PATH 的配置。以下以 WIndows 和 Linux 举例来说，假如这里解压后得到的文件夹为 scala-2.10.6。 Windows 环境假如 scala-2.10.6 文件夹放在了 C:\\目录下，那么新增 SCALA_HOME 的值为 C:\\scala-2.10.6 ，然后找到 PATH 这个环境变量， 在已有的值后面添加 ;%SCALA_HOME%\\bin （注意前面的分号），重新打开一个新的命令行窗口即可操作。关于windows环境变量的其他说明情自行补脑，这里就不多啰嗦了。 Linux 和 Mac 环境这里假定我们 scala-2.10.6 的 scala 目录存放在 /usr/local/ 下，绝对路径就是 /usr/local/scala-2.10.6 。 Bash 环境bash 环境下可以修改 ~/.bashrc 或者 /etc/profile 文件，添加一下内容： 1234SCALA_HOME=/usr/local/scala-2.10.6PATH=$PATH:$SCALA_HOME/binexport SCALA_HOME PATH 然后执行 source ~/.bashrc 或者 source /etc/profile 即可。 Zsh 环境如果你的终端bash用的是 zsh ，那么需要在 ~/.zshrc 文件中增加上述内容，然后执行 source ~/.zshrc 即可。 Sbt 配置官方地址，从这里下载，如果是MacOS的话，可以依照这里提示的方法进行快捷安装，如果是其他平台或者想手动配置，点击这里直接下载即可。 下载完成并解压，得到文件夹 sbt。 Windows 环境假如我们将 sbt 文件夹放到了 C:\\目录下。新增环境变量 SBT_HOME 值为 C:\\sbt， 在 PATH 变量值后面添加 ;%SBT_HOME%\\bin，重新打开一个新的命令行窗口即可。 Linux 或者 Mac 环境这里假定我们将 sbt 目录放在了 /usr/local/ 目录下。同上面配置 scala 环境变量一样。 Bash 环境编辑 ~/.bashrc 或者 /etc/profile ，新增以下内容: 1234SBT_HOME=/usr/local/sbtPATH=$PATH:$SBT_HOEM/binexport SBT_HOME PATH 然后执行 source ~/.bashrc 或者 source /etc/profile 即可生效。 Zsh 环境编辑 ~/.zshrc， 添加上面的内容并保存后，执行 source ~/.zshrc 即可生效。 示例程序项目结构在随意一个地方创建一个文件夹，名字为 spark-sbt-demo ，下面是目录结构： 123456789spark-sbt-demo ├── build.sbt├── project│ ├── build.properties│ └── plugins.sbt└── src ├── main ├── scala └── WordCount.scala build.sbt 文件中添加如下内容（注意每行要用空行隔开）： 1234567891011name := \"spark-sbt-demo\"version := \"1.0\"scalaVersion := \"2.10.6\"organization := \"spark.demo\"version := \"1.0.0-SNAPSHOT\"libraryDependencies += \"org.apache.spark\" % \"spark-core_2.10\" % \"1.6.1\" % \"provided\" WordCount.scala 文件内容： 12345678910111213import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by sdvdxl on 16/5/11. */object WordCount &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(\"spark-sbt-demo\").setMaster(\"local[*]\") val sc = new SparkContext(conf) sc.textFile(\"src/main/scala/WordCount.scala\").flatMap(_.split(\" \")).map(word=&gt;(word,1)).reduceByKey(_+_).foreach(println) sc.stop() &#125;&#125; build.properties 文件内容 ： 1sbt.version = 0.13.11 plugins.sbt 文件先保持为空。 至此，我们已经创建了一个 sbt 机构的项目。 接下来会说明使用sbt下载依赖，使用 idea 创建 sbt 项目，在idea中如何运行sbt管理的 spark app。 Sbt 的基本使用上面我们创建了一个用 sbt 管理的 spark app 项目，如果想要提交到spark中运行，那么需要打包成jar包，好在 sbt 本身或者插件提供了这样的功能。 应用打包打开命令行，切换到该项目目录下，然后输入 sbt 之后，进入 sbt 的交互中，然后输入 package ，开始打包，最后如果看到类似 [info] Done packaging. [success] Total time: 11 s, completed 2016-5-11 12:32:09 字样，那么说明打包成功，打成的 jar 包在上面的日志中可以找到。 第三方 jar 统一打包在写应用的时候，我们不只是用到 spark 自身的 jar 包，还会用到好多其他第三方类库，那么，在提交应用到 spark 运行的时候，这些第三方依赖也需要一并提交上去，否则会出现找不到类的问题。如果依赖少的话，直接将这些 jar 包直接一个一个提交上去也没问题，但是一旦依赖了大量的类库，这种方法显然是低效费力的，那么怎么才能将这些所有的第三方依赖打成一个 jar 包呢？ sbt 本身没有提供这样的功能，但是我们可以依靠相应的插件完成此操作。记得上面有个文件内容留空的 plugins.sbt 文件吗？这个文件中可以配置我们想要完成特定功能的插件，现在我们在其中添加如下内容： 1addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.2\") 然后重新 进入 sbt 交互式环境，输入 assemblyPackageDependency 回车，稍后将看到类似如下输出： [info] Done packaging. [success] Total time: 41 s, completed 2016-5-11 13:36:37 这样就成功的将所有依赖的第三方类库打包到一个 jar 包中了，具体打包的文件可以在上面的日志中看到。 使用 idea 创建 sbt 项目安装插件使用 idea 创建 sbt 项目需要安装 scala 和 sbt 插件。打开idea的首选项，然后找到 Plugins ，点击 Browser repositores... 按钮，输入 scala 搜索，然后找到 scala 和 sbt 的插件进行安装，如下图所示：安装完成后重启idea。 创建 sbt 项目File -&gt; New -&gt; Project… 打开项目创建向导：创建完成后，等待idea刷新项目，目录结构大体如下（project/project 和 target相关没有列出）： 12345678910111213141516spark-sbt-demo ├── build.sbt├── project│ ├── build.properties│ └── plugins.sbt└── src ├── main │ ├── java │ ├── resources │ ├── scala │ └── scala-2.11 └── test ├── java ├── resources ├── scala └── scala-2.11 plugins.sbt 文件放置插件配置 build.sbt 是整体的项目配置信息 build.properties 可以设置 sbt 版本 java 目录存放 java 文件 scala 目录存放 scala 文件 resources 目录用来存放配置文件 test 相关目录用来存放测试相关文件在 idea 中 运行 spark app上面我们介绍了如何使用 idea 项目向导创建一个 sbt 项目，现在我们来说一下如何在 idea 中直接运行 sbt 构建的 spark app。 这里我们使用一开始我们创建的那个项目，使用 idea 导入功能，File -&gt; Open 找到项目目录打开即可。在 WordCount.scala 文件中右键，选择 Run WordCount ，开始运行，但是结果可能不是我们所期望的： 12345678910111213141516Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/SparkConf at WorldCount$.main(WorldCount.scala:8) at WorldCount.main(WorldCount.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)Caused by: java.lang.ClassNotFoundException: org.apache.spark.SparkConf at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 7 moreProcess finished with exit code 1 这是为什么呢？原因是我们在 build.sbt 中配置的 spark 依赖是这样的： 1libraryDependencies += \"org.apache.spark\" % \"spark-core_2.10\" % \"1.6.1\" % \"provided\" 注意到后面的 provided 了吗？这个代表打包或者运行的时候不会将这个 jar 包的文件包含进去（注意：spark app 要求这样，注意不要把spark相关的jar包包含进去）。这样导致我们无法再 idea 中调试或者运行 spark app。 解决方案还是有的，sbt 和 maven（也是一个项目管理的软件）一样，提供了模块开发功能，我们定义两个模块，一个模块就是我们上面我们做好的，另一个是用来运行的，这个里面包含了运行时类库，配置如下： 创建一个名为 main 的文件夹，把项目中的 src 文件夹移动到这个目录下 在项目根目录下创建名为 run 的文件夹 修改项目根目录下的 build.sbt 文件，内容为： 1234567891011121314151617name := &quot;spark-sbt-demo&quot;version := &quot;1.0&quot;scalaVersion := &quot;2.10.4&quot;organization := &quot;spark.demo&quot;version := &quot;1.0.0-SNAPSHOT&quot;libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-core_2.10&quot; % &quot;1.6.1&quot; % &quot;provided&quot;lazy val root = (project in file(&quot;.&quot;)).aggregate(main, run)lazy val main = (project in file(&quot;main&quot;))lazy val run = (project in file(&quot;run&quot;)).dependsOn(main) 在子项目 main 创建 build.sbt 内容为： 1libraryDependencies += \"org.apache.spark\" % \"spark-core_2.10\" % \"1.6.1\" % \"provided\" 在子项目 run 创建 build.sbt 内容为 ： 1libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-core_2.10&quot; % &quot;1.6.1&quot; 配置运行参数，如下图：然后选择上面的运行配置，运行即可。这里可能会碰到一个异常： 1234Exception in thread \"main\" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/du/workspace/hekr/spark-sbt-demo/src/main/scala/WorldCount.scala at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:251) at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:270) ...... 这是由于上面我们修改了改程序main文件的位置，导致找不到该文件所致，请自行设置为一个存在的文件路径或者修改为 main/src/main/scala/WorldCount.scala 重新运行即可成功。 Sbt 本地依赖库存储位置配置抽空再补上，其实就是建立一个连接，先自行思考方案。 项目下载没有源码下载的都是耍流氓，点这里下载","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"JDK多版本管理","slug":"java/JDK多版本管理","date":"2016-04-20T01:43:43.000Z","updated":"2017-11-04T08:30:45.784Z","comments":true,"path":"2016/04/20/java/JDK多版本管理/","link":"","permalink":"http://yoursite.com/2016/04/20/java/JDK多版本管理/","excerpt":"","text":"在实际工作环境中经常碰到不同项目要用不同jdk版本问题，比如我的项目组现在用的是jdk8，而用spark打包的应用是用的jdk7，所以有必要记录一下版本配置和切换问题。","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://yoursite.com/tags/jdk/"}]},{"title":"通过SpringXD将信息分片|文件夹存储到HDFS","slug":"spring/通过SpringXD将信息分片-文件夹存储到HDFS","date":"2016-03-16T09:29:03.000Z","updated":"2017-11-04T08:30:45.791Z","comments":true,"path":"2016/03/16/spring/通过SpringXD将信息分片-文件夹存储到HDFS/","link":"","permalink":"http://yoursite.com/2016/03/16/spring/通过SpringXD将信息分片-文件夹存储到HDFS/","excerpt":"","text":"kafka --topic=kafka_test --zkconnect=10.10.1.20:2181 --queueSize=64 |hdfs --inputType=application/json --idleTimeout=10000 --partitionPath=dateFormat(&apos;yyyy/MM/dd/HH/mm&apos;)","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"},{"name":"spring xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"sparkApp提交到SpringXD出现错误可能情况及解决方法","slug":"spark/sparkApp提交到SpringXD出现错误可能情况及解决方法","date":"2016-03-16T03:52:19.000Z","updated":"2017-11-04T08:30:45.788Z","comments":true,"path":"2016/03/16/spark/sparkApp提交到SpringXD出现错误可能情况及解决方法/","link":"","permalink":"http://yoursite.com/2016/03/16/spark/sparkApp提交到SpringXD出现错误可能情况及解决方法/","excerpt":"","text":"编写spark代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.demo;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.ArrayList;import java.util.List;/** * Created by sdvdxl on 2016/3/14. */public class SparkCalcDemo &#123; private static final String HADOOP_URL = \"hdfs://10.10.1.110:8020/\"; public static void main(String[] args) throws Exception &#123; SparkConf conf = new SparkConf().setAppName(\"test\").setMaster(\"local[1]\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; textFile = sc.textFile(HADOOP_URL + \"/xd/dataset1/2016/03/14/15/01\", 1); JavaRDD&lt;String&gt; words = textFile.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; public Iterable&lt;String&gt; call(String s) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); JSONObject jobj = JSON.parseObject(new String(org.apache.commons.codec.binary.Base64.decodeBase64(s.substring(1, s.length() - 1)))); list.add(jobj.getString(\"name\")); list.add(jobj.getString(\"random\")); return list; &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; counts = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125; &#125;); counts.foreach(tuple2 -&gt; System.out.println(tuple2._1 + \" : \" + tuple2._2)); &#125;&#125; pom依赖： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;kafka-demo&lt;/groupId&gt; &lt;artifactId&gt;kafka-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.7&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;!--&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-scala_2.10&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;--&gt; &lt;/dependency&gt; &lt;!--dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-scala_2.10&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-base64&lt;/artifactId&gt; &lt;version&gt;2.16.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt; &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 项目资源插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-resources&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- 拷贝项目src/main/resources/下，除.bat以外的所有文件到conf/目录下 --&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/conf&lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources/&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;excludes&gt; &lt;exclude&gt;*.bat&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;copy-command&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;!-- 只拷贝项目src/main/resources/目录下的.bat文件到输出目录下 --&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;&lt;/outputDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources/&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;*.bat&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 打包插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;!-- 生成MANIFEST.MF的设置 --&gt; &lt;manifest&gt; &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;!-- jar启动入口类--&gt; &lt;mainClass&gt;com.some.package.some.class.Main&lt;/mainClass&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;!-- 在Class-Path下添加配置文件的路径 --&gt; &lt;Class-Path&gt;conf/&lt;/Class-Path&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;includes&gt; &lt;!-- 打jar包时，只打包class文件 --&gt; &lt;include&gt;**/*.class&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;oschina&lt;/id&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;mavenspring&lt;/id&gt; &lt;url&gt;http://maven.springframework.org/release&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;jcenter&lt;/id&gt; &lt;url&gt;http://jcenter.bintray.com&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;http://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-release&lt;/id&gt; &lt;name&gt;Spring Releases&lt;/name&gt; &lt;url&gt;http://repo.spring.io/libs-release&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;http://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/project&gt; 定义xd job: 1job create --name sparkAppDemo --definition &quot;sparkapp --mainClass=com.demo.SparkCalcDemo --appJar=/home/spark/spark-app.jar --master=local[1]&quot; --deploy 加载job 1job launch sparkAppDemo 然后出现以下类似的错误，主要是：...redis:queue-inbound-channel-adapter...错误 1234567891011121314151617181920212223242526272829303132333435362016-03-16T10:16:10+0800 1.3.1.RELEASE INFO DeploymentSupervisor-0 zk.ZKJobDeploymentHandler - Deployment status for job &apos;sparkAppDemo&apos;: DeploymentStatus&#123;state=deployed&#125;2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark application &apos;com.demo.SparkCalcDemo&apos; finished with exit code: 12016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:284)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:238)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.util.jar.JarVerifier.update(JarVerifier.java:228)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.util.jar.JarFile.initializeVerifier(JarFile.java:383)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.util.jar.JarFile.getInputStream(JarFile.java:450)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.JarIndex.getJarIndex(JarIndex.java:137)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.security.AccessController.doPrivileged(Native Method)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath$JarLoader.&lt;init&gt;(URLClassPath.java:803)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath$3.run(URLClassPath.java:530)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath$3.run(URLClassPath.java:520)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.security.AccessController.doPrivileged(Native Method)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath.getLoader(URLClassPath.java:519)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath.getLoader(URLClassPath.java:492)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath.getNextLoader(URLClassPath.java:457)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at sun.misc.URLClassPath.getResource(URLClassPath.java:211)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.net.URLClassLoader$1.run(URLClassLoader.java:365)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.net.URLClassLoader$1.run(URLClassLoader.java:362)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.security.AccessController.doPrivileged(Native Method)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.net.URLClassLoader.findClass(URLClassLoader.java:361)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.lang.ClassLoader.loadClass(ClassLoader.java:424)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.lang.ClassLoader.loadClass(ClassLoader.java:357)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.lang.Class.forName0(Native Method)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at java.lang.Class.forName(Class.java:348)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:538)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)2016-03-16T10:16:14+0800 1.3.1.RELEASE ERROR inbound.job:sparkAppDemo-redis:queue-inbound-channel-adapter1 tasklet.SparkTasklet - Spark Logger: Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties 仔细看的话，在上面有一句Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes错误，这个错误是由于导出的jar包结构信息不正确导致的。用eclipse的导出runnable jar 功能导出的jar包就没问题了。 另外如果有依赖的jar包没哟被加载进去，则会在最上方出现java.lang.NoClassDefFoundError:类似信息。 相关资料：以分布式方式运行Spring-XD","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"spring xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"}]},{"title":"linux下根据端口号查询对应进程","slug":"linux/linux下根据端口号查询对应进程","date":"2016-03-15T10:22:37.000Z","updated":"2017-11-04T08:30:45.786Z","comments":true,"path":"2016/03/15/linux/linux下根据端口号查询对应进程/","link":"","permalink":"http://yoursite.com/2016/03/15/linux/linux下根据端口号查询对应进程/","excerpt":"","text":"适用于ip4 1lsof -Pnl +M -i4 | grep port 适用于ip6 1lsof -Pnl +M -i6 | grep port","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"端口","slug":"端口","permalink":"http://yoursite.com/tags/端口/"},{"name":"进程","slug":"进程","permalink":"http://yoursite.com/tags/进程/"}]},{"title":"golang打印错误栈信息","slug":"golang/golang打印错误栈信息","date":"2016-03-15T10:21:05.000Z","updated":"2017-11-04T08:30:45.778Z","comments":true,"path":"2016/03/15/golang/golang打印错误栈信息/","link":"","permalink":"http://yoursite.com/2016/03/15/golang/golang打印错误栈信息/","excerpt":"","text":"12345678910111213141516package mainimport ( \"runtime\" \"fmt\")func main() &#123; outer()&#125;func outer() &#123; inner()&#125;func inner() &#123; defer func() &#123; if err := recover(); err != nil &#123; trace := make([]byte, 1024) count := runtime.Stack(trace, true) fmt.Printf(\"Recover from panic: %s\\n\", err) fmt.Printf(\"Stack of %d bytes: %s\\n\", count, trace) &#125; &#125;() panic(\"Fake error!\")&#125;","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"golang容易忽略导致入坑的方面","slug":"golang/golang容易忽略导致入坑的方面","date":"2016-03-15T10:19:19.000Z","updated":"2017-11-04T08:30:45.778Z","comments":true,"path":"2016/03/15/golang/golang容易忽略导致入坑的方面/","link":"","permalink":"http://yoursite.com/2016/03/15/golang/golang容易忽略导致入坑的方面/","excerpt":"","text":"main 函数main函数必须定义在main包里，不可导出。当main函数调用完毕，程序就会立即退出，不会等待运行中的goroutine运行完毕。 初始化函数func init() { … } 此函数无参，不需显示调用。 此函数根据依赖顺序执行初始化顺序，并且是顺序执行，如： A 依赖 B， B 依赖 C，并且A，B，C 都有init函数，那么初始化顺序是C ，B ， A iota iota 只能用在const定义的常量上 iota在一个const范围，不管中间的变量有没有用到iota，iota都是持续递增加1的，如 123456789101112const ( aa = iota bb cc = 3 dd = iota) dd 是3 如果重新定义const，那么值会是重新从0开始，如 123456const ( ee = iota) 类型断言x.(T) x为变量， T为要判断的类型，其中有两种用法 a := x.(T) ，如果x是类型T的实现类型变量，那么可以得到转换后的值a，否则panic: interface conversion: (x的类型) is not package.T: missing method （T的方法） a, ok := x.(T)，如果x是类型T的实现的类型变量，那么ok为true， 转换成功，否则，ok为false，说明转换失败。 从Panic中恢复需要用到build-in recover()方法，函数原型： func recover() interface{} 要捕获panic，必须在有可能发生panic的地方的上面进行处理， 12345678910defer func()&#123; if err := recover(); err !=nil &#123; //处理错误 &#125;&#125;() 一般来说，会将上面的代码放在函数（或者方法）的开始的地方。 注意 ， recover函数只能用在defer中，否则不会进行错误捕获。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"sink-hdfs","slug":"spring/sink-hdfs","date":"2016-03-14T08:10:23.000Z","updated":"2017-11-04T08:30:45.790Z","comments":true,"path":"2016/03/14/spring/sink-hdfs/","link":"","permalink":"http://yoursite.com/2016/03/14/spring/sink-hdfs/","excerpt":"","text":"hdfs根据时间自动划分文件夹stream create --name dataset1 --definition &quot;kafka --topic=kafka_test --zkconnect=10.10.1.20:2181 --queueSize=64 |hdfs --inputType=application/json --idleTimeout=10000 --partitionPath=dateFormat(&#39;yyyy/MM/dd/HH/mm&#39;)&quot; --deploy其中，–partitionPath=dateFormat(‘yyyy/MM/dd/HH/mm’)用来指定划分 策略，这个是说用年(四位)/月(两位)/天(2位)/时(2位)/分(2位)这种格式来划分","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"},{"name":"spring xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"golang命令","slug":"golang/golang命令","date":"2016-03-13T04:00:37.000Z","updated":"2017-11-04T08:30:45.777Z","comments":true,"path":"2016/03/13/golang/golang命令/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/golang命令/","excerpt":"","text":"Go 提供了很多好用的命令，比如可以用go get 直接从网络上导入包，下面介绍一些常用的Go命令 go get基本用法go get 命令用来直接下载并安装网络上的包到GOPATH中。事实上它依赖于版本控制工具，比如常用的Git和Mercurial。比如我们要使用github.com/golang/text这个包，那么可以执行命令 go get github.com/golang/text，稍后片刻（依照网络情况时间长短不一），那么这个包源码就会放到 GOPATH/src对应的目录下，编译好的文件会放到GOPATH/pkg对应的目录下，如果有可执行性代码，那么会将编译好的可执行性文件放到GOPATH/bin目录下。 高级用法像刚才例子中提到的github.com/golang/text这个包下又有子包，也想get下来，该如何做呢？也许你想没办法，一个一个get吧，的确这是一个不错的方法，但是有一个更为高效和优雅的方式，那就是 ...，这个符号代表旗下子目录，那我们可以这样操作go get github.com/golang/text ...，就可以将text和其子包同时get下来了。 参数 -v 可以显示正在get的包 -x 可以显示正在执行的命令go build编译go文件。可以切换到要build的包中执行go build，也可以直接 “go build” + “包名”，如要build包 github.com/golang/text，可以切换到text包目录下执行 go build，也可以直接执行 go build github.com/golang/text，这样就可以build text包了，如果要build子包，那么输入go build ./...。build命令会将main函数编译成可执行性文件，如果没有main，那么没有额外的文件产生。 go cleanclean和build作用相反，是将build出来的可执行性文件清除掉。 go installinstall 命令是将包编译成二进制文件并放到GOPATH/pkg/目标平台/对应的目录下。比如我们自己从github上克隆了github.com/golang/text，要使用它的话，需要进入到text包中，然后执行 go install；加入有子包也要安装的话，输入go install ./...，就会在GOPATH/pkg/目标平台/产生后缀为.a的文件。当然也可以和build一样输入完整的包路径。 go list这个命令用来查看所引用的包。 基本用法 直接跟包名，如：github.com/golang/text，会打印这个包本身；也可以切换到该包目录下执行go list，效果相同。 可以直接跟”包名”+”/…”，比如要查看github.com/golang/text这个包的导入情况，执行go list github.com/golang/text/...会打印本包的go文件。参数-f可以指定打印格式，默认是go list -f &#39; {{.ImportPath}} &#39;， -f后面的参数可以用下面结构体的属性： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 type Package struct &#123; Dir string // directory containing package sources ImportPath string // import path of package in dir ImportComment string // path in import comment on package statement Name string // package name Doc string // package documentation string Target string // install path Goroot bool // is this package in the Go root? Standard bool // is this package part of the standard Go library? Stale bool // would 'go install' do anything for this package? Root string // Go root or Go path dir containing this package // Source files GoFiles []string // .go source files (excluding CgoFiles, TestGoFiles, XTestGoFiles) CgoFiles []string // .go sources files that import \"C\" IgnoredGoFiles []string // .go sources ignored due to build constraints CFiles []string // .c source files CXXFiles []string // .cc, .cxx and .cpp source files MFiles []string // .m source files HFiles []string // .h, .hh, .hpp and .hxx source files SFiles []string // .s source files SwigFiles []string // .swig files SwigCXXFiles []string // .swigcxx files SysoFiles []string // .syso object files to add to archive // Cgo directives CgoCFLAGS []string // cgo: flags for C compiler CgoCPPFLAGS []string // cgo: flags for C preprocessor CgoCXXFLAGS []string // cgo: flags for C++ compiler CgoLDFLAGS []string // cgo: flags for linker CgoPkgConfig []string // cgo: pkg-config names // Dependency information Imports []string // import paths used by this package Deps []string // all (recursively) imported dependencies // Error information Incomplete bool // this package or a dependency has an error Error *PackageError // error loading package DepsErrors []*PackageError // errors loading dependencies TestGoFiles []string // _test.go files in package TestImports []string // imports from TestGoFiles XTestGoFiles []string // _test.go files outside package XTestImports []string // imports from XTestGoFiles&#125; 如果想打印比较全的信息，也有一个参数可以使用：-json，这会将上面部分信息以json格式打印出来，读者可以自行实验。 go run这个命令可以直接运行go文件（带main函数），不需要编译成二进制可执行性文件。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"Android通话自动录音","slug":"android/Android通话自动录音","date":"2016-03-13T03:45:09.000Z","updated":"2017-11-04T08:30:45.772Z","comments":true,"path":"2016/03/13/android/Android通话自动录音/","link":"","permalink":"http://yoursite.com/2016/03/13/android/Android通话自动录音/","excerpt":"","text":"应用介绍：通话自动录音，包括去电/来电接通录音。 启动服务之后，当电话接通之后开始录音, 录音文件保存在存储卡上My Record文件夹内。 同时可以用本软件进行查看录音文件，播放，删除等操作。 单击录音文件可以长按录音文件可以单个删除 版本：1.5百度应用地址百度应用同类搜索占位为第一位 应用截图","categories":[{"name":"android","slug":"android","permalink":"http://yoursite.com/categories/android/"}],"tags":[{"name":"android","slug":"android","permalink":"http://yoursite.com/tags/android/"},{"name":"安卓","slug":"安卓","permalink":"http://yoursite.com/tags/安卓/"},{"name":"录音","slug":"录音","permalink":"http://yoursite.com/tags/录音/"}]},{"title":"linux工具集锦","slug":"linux/linux工具集锦","date":"2016-03-13T03:44:10.000Z","updated":"2017-11-04T08:30:45.786Z","comments":true,"path":"2016/03/13/linux/linux工具集锦/","link":"","permalink":"http://yoursite.com/2016/03/13/linux/linux工具集锦/","excerpt":"","text":"bcompareBeyond Compare是一套超级的文件及文件夹(目录)的比较工具，不仅可以快速比较出两个目录的不同，还可以比较每个文件的内容，而且可以任意显示比较结果。程序内建了文件浏览器，方便您对文件、文件夹、压缩包、FTP网站之间的差异比对以及资料同步。使用它可以管理源代码，保持文件夹的同步，比较程序输出，及验证光盘的复制。它还支持脚本、插件，尤其对中文支持很好。 GeanyEvincepdf阅读器 #Markdown 编辑器Farbox tmux一款可以在原有的终端上进行分屏工作的工具。 mutate类似 mac上的alfred的软件，地址ppa安装方式123sudo add-apt-repository ppa:mutate/ppasudo apt-get updatesudo apt-get install mutate plank一款友好实用的dock工具。 virtualbox一款跨平台的可以安装任何操作系统的虚拟机软件。官方地址，同时最好安装扩展，可以有增强功能（拷贝，拖拽，更好的显示）。 xfce zhuti123sudo add-apt-repository ppa:rebuntu16/other-stuffsudo apt-get updatesudo apt-get install xfce-theme-manager","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"软件","slug":"软件","permalink":"http://yoursite.com/tags/软件/"}]},{"title":"配置Idea的Go开发环境","slug":"golang/配置Idea的Go开发环境","date":"2016-03-13T03:43:19.000Z","updated":"2017-11-04T08:30:45.781Z","comments":true,"path":"2016/03/13/golang/配置Idea的Go开发环境/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/配置Idea的Go开发环境/","excerpt":"","text":"获取IDEA 最新版戳这里下载，下载对应平台的版本，一般来说，社区版(Community Edition)就已经足够了。 安装IDEA。Windows和Mac是安装版，一步一步安装完成即可；Linux是免安装版，解压，给IDEA的执行文件添加可执行权限即可。假设安装目录是H:\\software\\dev\\JetBrains\\IntelliJ IDEA Community Edition 14.1.1，其他平台自行设置目录。 获取Go Go的官方网站是http://golang.org/，下载地址，但是鉴于中国网络问题，不科学上网则没法下载，各位同学可以从这里下载，最好下载最新的，当然这个网站(http://golangtc.com/)本身就是国内较活跃的一个Go社区。 下载后解压到本地目录，假设安装目录是H:\\software\\dev\\go。其他平台自行设置目录。 获取Git和hg因为Go get命令要使用到git或者hg，所以需要安装git和hg。git可以从这里下载, hg(执行命令是hg，实际下载的软件叫mercurial)可以从这里下载，加入git安装到H:\\software\\dev\\Git，hg安装到H:\\software\\dev\\Mercurial 配置Go和Git(hg)配置环境变量，这里以Windows为例，其他平台请自行换成对应的路径即可。如果打开命令行分别执行以下命令都成功，那么不需要额外配置环境变量，否则配置对应的环境变量123go versiongit versionhg version 变量名称 变量值 说明 GOROOT H:\\software\\dev\\go go根路径 GOPATH H:\\software\\dev\\gopath gopath可以是任何一个目录 PATH %PATH%;%GOROOT%\\bin;H:\\software\\dev\\Git\\bin;H:\\software\\dev\\Mercuria; 注意不要忘了加入原来的path变量 配置IDEA的Go环境 打开IDEA，File -&gt; Settings -&gt; Plugins -&gt; Browse repositiores… -&gt; Manage repositories… ，添加自定义repository url https://plugins.jetbrains.com/plugins/nightly/list(nightly build)或者 https://plugins.jetbrains.com/plugins/alpha/list(alpha version)，添加完成之后，等待刷新完成后，输入go，选择go插件，点击安装，等待安装完成后，重启生效。网络环境不好的话，可能插件不能下载，可以直接去idea官网下载插件，如何获取最新插件呢，这里是根据updateId来的，这个最新的id就是从上面的repository的url中获取的，用浏览器打开这个url，就会观察到以下内容 12345678910111213141516171819&lt;idea-plugin downloads=\"97922\" size=\"1071401\" date=\"1428797441000\" url=\"\"&gt;&lt;name&gt;Go&lt;/name&gt;&lt;id&gt;ro.redeul.google.go&lt;/id&gt;&lt;description&gt;&lt;![CDATA[Support for Go programming language. &lt;p&gt;Alpha pre-release of the 1.0.0 version.&lt;/p&gt; &lt;p&gt;Doesn't contain all the functionality of the 0.9.x branch but has a completely reworked internals. It's faster than 0.9.x, refactoring works to some degree and has native support for gopath packages.&lt;/p&gt; Compatibility &lt;p&gt;Plugin can be installed on IntelliJ platform 141.2 or greater. It corresponds to IntelliJ IDEA 14.1, WebStorm 10, PhpStorm 9&lt;/p&gt;]]&gt;&lt;/description&gt;&lt;version&gt;0.9.271&lt;/version&gt;&lt;vendor email=\"\" url=\"https://github.com/go-lang-plugin-org\"/&gt;&lt;download-url&gt;../../plugin/download?updateId=19402&lt;/download-url&gt;&lt;idea-version min=\"n/a\" max=\"n/a\" until-build=\"3999\"/&gt;&lt;change-notes&gt;&lt;![CDATA[&lt;ul&gt; &lt;li&gt;Initial GAE support: running dev server. &lt;strong&gt;Requires resetting project SDK.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;]]&gt;&lt;/change-notes&gt;&lt;rating&gt;4.3&lt;/rating&gt;&lt;/idea-plugin&gt; 其中 download-url 中的 updateId 就是最新的下载id。如果还是没法下载，那么请点击这里，从百度云上下载。然后 File -&gt; Settings -&gt; Install plugin from disk…选择刚才下载的压缩包（不要解压），确定后重启成效。 File -&gt; Other settings -&gt; Default Project Structure… -&gt; Platform Settings -&gt; SDKs -&gt; + -&gt; Go SDK -&gt; 选择GOROOT路径，确定。 File -&gt; New Project -&gt; Go -&gt; Next -&gt; 输入Project name和Project location -&gt; Finish -&gt; 在项目根目录中新建main.go，添加以下内容 12345678 package main import ( \"fmt\") func main() &#123; fmt.Println(\"hello world\")&#125; Run -&gt; Edit Configrations -&gt; + -&gt; Go Application -&gt; File 中在原来的路径基础上添加main文件即 添加 \\main.go，点击确定，然后运行可以看到控制台打印 hello world。 配置GDB debugRun -&gt; Edit Configrations -&gt; Defaults -&gt; Go GDB -&gt;Name：可以随便填写GDB executeable：dbg.exe的完整路径Application executable：填写生成的可执行文件的完整路径，路径要是windows写法，如G:\\gopath\\src\\example\\main.exe则需要两个反斜杠，就变成了G:\\gopath\\src\\example\\main.exe，或者是Unix写法 G:/gopath/src/example/main.exe,否则会提示找不到文件。这里的可执行文件必须使用go build -gcflags &quot;-N -l&quot;编译出来的，这样的文件带有debug信息并且没有被go内联优化。 配置保存时自动格式化代码和自动导入这个配置需要用到IDEA的宏（所谓的宏，就是一系列操作），下面就说怎么录制这个宏。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"go读取文件","slug":"golang/go读取文件","date":"2016-03-13T03:42:18.000Z","updated":"2017-11-04T08:30:45.781Z","comments":true,"path":"2016/03/13/golang/go读取文件/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/go读取文件/","excerpt":"","text":"使用File不多说，直接上代码 1234567891011121314151617181920212223242526272829303132func readUseFile() &#123; file, err := os.Open(\"f:/file.txt\") handleError(err) defer file.Close() buf := make([]byte, 512) for &#123; n, err := file.Read(buf) //1 // if err != nil &amp;&amp; err == io.EOF &#123; // break // &#125; //2 // if n == 0 &#123; // break // &#125; //3 if n == 0 &amp;&amp; err != nil &amp;&amp; err == io.EOF &#123; break &#125; fmt.Print(n) fmt.Print(string(buf)) &#125; handleError(err)&#125; 可以看到，file本身具有读取文件内容的函数，入参事一个切片，是数据的缓冲区，出参第一个是实际读取的大小，第二个是读取过程中发生的错误。如果有数据且读取成功，则n&gt;0,如果恰好读到文件末尾，则n=0。如果读取过程中有错误发生，则err不为nil，如果读取正常且读到了文件末尾，则err为io.EOF。 读取过程中有三种方法可以跳出死循环。第一种方法是判断err状态，如果不为nil且是io.EOF，则已经读取完毕；第二种方法是判断实际读取的数量，如果读取的量为0，则认为已经读取结束。第三种方式是上面两种的结合，这种判断要比上面两种中仁和一种都要保险，缺点就是罗嗦点。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"go交叉编译","slug":"golang/go交叉编译","date":"2016-03-13T03:41:45.000Z","updated":"2017-11-04T08:30:45.781Z","comments":true,"path":"2016/03/13/golang/go交叉编译/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/go交叉编译/","excerpt":"","text":"1.5之前需要进入go安装目录下的src目录，然后执行GOOS=windows GOARCH=i386 CGO_ENABLED=0 ./make.bash --no-clean 1.5及其之后可以直接执行CGO_ENABLED=0 GOOS=windows GOARCH=386 go build -o 输出文件名 go文件其中GOOS有：windows,linux,darwin也就是mac系统GOARCH有：adm64和386分别对应64位平台和32位平台","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"go中main","slug":"golang/go中main","date":"2016-03-13T03:16:46.000Z","updated":"2017-11-04T08:30:45.780Z","comments":true,"path":"2016/03/13/golang/go中main/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/go中main/","excerpt":"","text":"想必很多朋友在入门的时候都是拿main开始，而不是test，我也喜欢这样，我想可能是main比较为人熟知的用法吧，test在go中也是非常友好的，不需要依赖其他库就可方便使用。既然都偏向于main方法的开始和入门，那么这个博文就说一下go语言main相关的事情。原本只打算写一下main包的拆分和运行方式，突然想到还有其他一些注意地方，那么就一并记录一下，其他的如果使用过程中遇到了，再进行记录。以下示例都是在GOPATH下进行 main函数定义想要作为程序的运行入口，那么这个函数必须明明为main，同时要放到main包。main函数声明极其简单，如下：123func main()&#123; //...&#125; 这样就声明并定义好了程序的运行的入口函数，不需要其他额外的参数和返回值。加上包的声明，完整的main文件就如下格式：123package mainfunc main() &#123;&#125; 另外main文件名字可以随意命名，只需要后缀是.go就可以了，如：a.go, main.go, server.go等都是可以的。 main 的运行要运行main文件，go提供了2中方式： go run main.go 其中main.go 就是要运行的main函数所在的文件 go build命令，文件名可以省略，也可以加上， 还可以用用参数 -o 指定编译后的文件，如 go build -o main.exe main.go 就是把main.go文件编译成main.exe可执行文件，然后直接执行main.exe就可以运行了。 main 包文件（内容）的拆分假如觉得一个main文件中放太多东西有点杂乱，那么可以把main函数和其他内容拆开，放到不同的文件中（这里指的都是main这个包中），文件名字随意。比如我们有个sum函数，那么可以把它放到math.go这个main包的文件中，然后main函数独立在main.go文件中，main方法可以直接调用main包（相同名字的包）的函数、方法或者变量。那么如何执行呢，当然就可以用上述的方法运行。但是如果用go run main.go这种方式，那么可能遇到一个问题：找不到sum这个函数。因为run的只是main.go 这一个文件，没有加载其他文件的内容，自然就找不到sum这个函数了，那么加载sum这个函数内容，自然就可以了，对应执行方式就变成了go run main.go math.go，也就是说，要把相关以来的内容相关的文件也要加到run 后面。用go build 如果不指定文件名字，那么go会自行加载依赖项目，可以顺利执行。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"golang入门","slug":"golang/golang入门","date":"2016-03-13T03:12:45.000Z","updated":"2017-11-04T08:30:45.775Z","comments":true,"path":"2016/03/13/golang/golang入门/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/golang入门/","excerpt":"","text":"本文同步发表于我的简书,点此穿越 Go 基本介绍Go发展早在2007年9月，Go语言还是这帮大牛的20%自由时间的实验项目。 幸运的是， 到了2008年5月 ， Google发现了Go语言的巨大潜力， 从而开始全力支持这个项目 ， 让这批人可以全身心投入Go语言的设计和开发工作中。 Go语言的第一个版本在2009年11月正式对外发布，并在此后的两年内快速迭代，发展迅猛。 第一个正式版本的Go语言于2012年3月28 日正式发布， 让Go语言迎来了第一个引人瞩目的里程碑。截至现在，Go已经更新到1.5版本，1.5正式版就在8月份中旬发布。Go编程语言是一个使得程序员更加有效率的开源项目。Go 是有表达力、简洁、清晰和有效率的。它的并行机制使其很容易编写多核和网络应用，而新奇的类型系统允许构建有弹性 的模块化程序。Go 编译到机器码非常快速，同时具有便利的垃圾回收和强大的运行时反射。它是快速的、静态类型编译语言，但是感觉上是动态类型的，解释型语言。 Go语言最主要的特性： 自动垃圾回收 更丰富的内置类型 函数多返回值 错误处理 匿名函数和闭包 类型和接口 并发编程 反射 语言交互性 开发环境配置请参见另一篇博客Golang 环境搭建 下面切入正题，介绍Go语言编程，由于只是本篇只是一个快速了解Go，所以有些内容会略微一提，如果读者用到或者要深入了解，可自行找文档参考，这里有个印象即可。 Go 编程基础基础数据类型 布尔类型：bool 整数类型：int8 uint8 int16 uint16 int32 uint32 int64 uint64 int rune byte complex128 complex64，其中，byte 是 int8 的别名 浮点类型：float32 float64 字符串类型：string 字符类型： rune，是 int32 的别名 空： nil 万能类型： interface{}操作符+ &amp; += &amp;= &amp;&amp; == != ( )- | -= |= || &lt; &lt;= [ ]* ^ *= ^= &lt;- &gt; &gt;= { }/ &lt;&lt; /= &lt;&lt;= ++ = := , ;% &gt;&gt; %= &gt;&gt;= -- ! ... . :&amp;^ &amp;^= -&gt;内置函数 len：计算（字符串，数组或者切片，map）长度 cap：计算（数组或者切片，map）容量 close：关闭通道 append：追加内容到切片 copy：拷贝数组/切片内容到另一个数组/切片 delete：用于删除map的元素 变量的定义和赋值 先定义，后赋值。变量的定义要用var 关键字声明，如,var str string，这就定义了一个名字为“str”的string类型的变量；（有过其他编程语言经验的读者可能会有点不适应，不过没错，Go的变量类型就是放在变量后面的。）还可以一次定义多个变量，如 var a string, b int;这样就同时定义了一个字符串类型和一个int类型的变量；如果几个连续的变量是同样的类型，可以一次性在最后该类型变量后说明，不需要单个说明。var a, b,c string, int d。也可以多行分别定义，如： 123var a stringvar b stringvar c int 这种写法要重复写var这个关键字，其实这种写法是可以只写一个var的，等价于下面的写法： 1234var ( a, b string c int) 变量的赋值var a string,定义一个string类型的a变量，然后a = &quot;this is a string&quot;就可以把字符串的值赋给a了。这里有个简便的写法，就是声明和赋值同时进行，以上两句等同于var a string = &quot;this is a string&quot;,这种写法大多数语言都是类似的.由于Go可以根据变量的值自动推断该变量的数据类型，所以还等价于var a = &quot;this is a string&quot;；另外Go中还有个更为简洁的写法，等同于a := &quot;this is a string&quot;，直接省略关键字var，取而代之的是一个操作符：=，这个操作符的作用就是声明并赋值。 常量的定义和赋值常量用关键词const说明，并且常量的值是在定义的时候一次性赋值的，如定义一个字符串常量，const CONST_STR = &quot;const string&quot;等价于const CONST_STR string = &quot;const string&quot;。 *注意const，var和:=不可同时使用。 函数函数的结构如下 func func_name([param_name type][...]) [return_value type[...]]。以关键字func开头， 后面是函数名， 函数名后面是函数参数，参数个数大于等于0个，参数后面是返回值，返回值个数&gt;=0，也就是说，Go语言支持多返回值。其中go中有个特殊的函数(其实还有一个init函数，但是作为入门篇，不在这里介绍了)，那就是main函数，main函数是无参，无返回值，名字是main的一个特殊函数，它是程序的入口，并且main函数只能定义在mian的包（下面有介绍）中。 包如果您是Java开发者，想必对包的概念并不陌生。在Go语言中，如果开发中有来自不同库的同名的函数，该如何处理，这就要依靠package来区分，也就是说包的作用类似于作用于，是对函数，变量等作用范围的一种约束。 包的定义包的定义是通过 package这个关键字来说明的，一般写在文件的最上方。如package a，则定义了一个名字为a的包。包的名字只允许有一级目录，即不允许类似java的com.example.a或者com/example/a这样的多层级定义。一般来说，包的名字最好和其父目录的名字一致，这样在使用包和包里的内容时会比较容易理解（另会有文章说明）。 包的使用包的导入使用关键字import来声明，如要在其他包中导入上面定义的包a，则声明如下：import &quot;a&quot;，如果多个，可以类似定义变量： 1234import ( \"a\" \"b\") 函数和变量（常量）导出规则Go语言中，抛弃了类似C++和Java中的private，public，protected，或者是friendld的可见性定义，采取了极简方式。如果变量或者函数首字母大写，代表可以导出，即对其他包是可见的，否则是不可见的。 到这里，Go的基本概念已经基本讲清楚，下面用一个示例说明上面的概念。 创建一个文件夹，名字假设叫做 example。 进入example，创建文件main.go,并添加以下内容： 12345678910111213141516package main //因为这里有程序的入口，main方法存在，所以包名必须命名为main// 这里导入要使用的包import ( \"./number\" //这里导入我们自己定义的包，“./”是说用相对路径的方式导入包 \"fmt\" //fmt是go标准包，用于处理输入输出)//这是程序的主函数，写法就是这样固定的，无参，无返回值func main() &#123; //调用我们的函数并把产生的结果赋值给定义的max和min变量，根据GetMaxAndMin的返回值类型，go自动推断出max和min的是int类型 max, min := number.GetMaxAndMin(1, 3, 4, 2, 6, 0, 8) //调用格式话输出打印max和min fmt.Printf(\"max:%v, min:%v\\n\", max, min)&#125; 创建文件numbner.go，并添加如下内容： 123456789101112131415161718192021222324252627282930313233343536373839// 这里可以添加注释，这是单行注释/* 这里也是注释， 是多行注释， 允许注释跨行*/package number// 这是包名，除了文件的注释，一般包名要放在最上方// 这里定义一个函数，用来获取values参数中的最大值和最小值。// 函数名GetMaxAndMin大写代表其他包可见，如果是小写，则其他包不可引用该函数// (a, b int, values ...int)，a，b, values都是该函的参数，a,b是必填的参数，// ...代表values是变参，即长度不固定，个数&gt;=0,并且都是int类型// (int, int)代表该函数有两个返回值，都是int类型func GetMaxAndMin(a, b int, values ...int) (int, int) &#123; max, min := a, a //定义并赋值两个变量 max，min，并把a的值赋给max和min if a&lt;b &#123; //go允许简单条件和控制语句之间不加小括号，并且大括号左部分必须和条件在同一行 max = b &#125; else &#123; min = b &#125; // 这里是for循环，在go中for是非常给力的循环控制器，没有其他方式（不推荐用goto） // _ 的作用是说把遍历values产生的下标的值忽略掉，v是产生的values的值， // range 关键词 用来配合for，构成一个简单的循环结构，相当于for-each // 关于 “_”，因为go中不允许有多余的为使用的参数和为使用的包，所以“_”就充当了一个垃圾桶的角色，多返回值产生的不必要值可以填入“_”, // 从而达到控制编译器编译过程中不会报错。 for _, v := range values &#123; if v&gt;a &#123; max = v &#125; if v&lt;b &#123; min = v &#125; &#125; return max, min&#125; 注意，文件保存编码是utf-8 。打开终端（命令行工具），切换到example目录下，执行go run main.go,可以看到有内容输出：&gt;max:8, min:0 附Go命令","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"golang-http-client-post","slug":"golang/golang-http-client-post[golang]","date":"2016-03-13T03:11:55.000Z","updated":"2017-11-04T08:30:45.774Z","comments":true,"path":"2016/03/13/golang/golang-http-client-post[golang]/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/golang-http-client-post[golang]/","excerpt":"","text":"title: Golang环境搭建date: 2016-03-13 11:10:48tags: go golangcategory: golang 下载Gogo官方网站在大陆已经被和谐，要访问，如果没有梯子，这里有个传送门,可以在线代理访问。首先就是要下载go开发程序了，建议在此处下载对应版本。 配置Go环境## Window 环境 下载对应的windows版本（注意64位和32位系统），然后解压得到go目录，假如名字就叫go，绝对路径是E:\\go。右键计算机(xp 是我的电脑，windows8是这台电脑)，选择属性，选择左侧的高级系统设置，接下来选择环境变量，出现环境变量的对话框。上面是当前用户的环境变量，也就是说配置的变量只是针对当前用户生效；下面是系统变量，对于整个系统的所有用户生效。我习惯于配置成系统变量，在此也是用系统变量举例。点击系统变量下的新建按钮，变量名填写 GOROOT,变量值填写上面解压后的go路径，在这也就是 E:\\go，然后点击确定。这个变量是用来配置go的home目录。再点击新建按钮，变量名填写GOPATH，变量值填写E:\\gopath(gopath要存在，当然也可以选择其他文件夹)，点击确定。这个变量是用来指定go查找包的路径，也是用go get 命令所需安装的位置目录。然后找到PATH变量，点击编辑，变量值最后添加一个半角分号，然后再输入 %GOROOT%\\bin，点击确定。可以关闭所有窗口了。在开始菜单中打开cmd命令行（也可以用快捷键，windows键+R，然后输入cmd回车）。在窗口中输入go回车，如果有go相关的帮助打印，则说明配置成功，否则没有成功，重新校验上面的环境变量配置是否正确。 ## Linux 环境 假设对应版本的go解压后绝对路径是/home/user/go。 vi（或者你喜欢的其他编辑器）打开 ~/.bashrc文件，在后面加入 &gt; GOROOT=/home/user/go GOPATH=/home/user/gopath PATH=$PATH:$GOROOT/bin export GOROOT GOPATH PATH 然后 运行命令 `soure ~/.bashrc` 输入 go ，如果打印go帮助文档，说明配置成功，否则检查环境变量配置是否正确。 编写第一个Go程序用你喜欢的编辑器编辑一个文件，加入文件名叫 hello.go 。敲入以下代码1234567package mainimport (\"fmt\")func main()&#123; fmt.println(\"Hello World!\")&#125; 然后再该文件当前目录下，输入 go build hello.go，在目录下会生成一个 hello(windows下是hello.exe)文件，运行该文件，可以看到控制台输入出 HelloWorld,运行成功。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"},{"name":"http","slug":"http","permalink":"http://yoursite.com/tags/http/"}]},{"title":"Golang环境搭建","slug":"golang/Golang环境搭建","date":"2016-03-13T03:10:48.000Z","updated":"2017-11-04T08:30:45.773Z","comments":true,"path":"2016/03/13/golang/Golang环境搭建/","link":"","permalink":"http://yoursite.com/2016/03/13/golang/Golang环境搭建/","excerpt":"","text":"下载Gogo官方网站在大陆已经被和谐，要访问，如果没有梯子，这里有个传送门,可以在线代理访问。首先就是要下载go开发程序了，建议在此处下载对应版本。 配置Go环境## Window 环境 下载对应的windows版本（注意64位和32位系统），然后解压得到go目录，假如名字就叫go，绝对路径是E:\\go。右键计算机(xp 是我的电脑，windows8是这台电脑)，选择属性，选择左侧的高级系统设置，接下来选择环境变量，出现环境变量的对话框。上面是当前用户的环境变量，也就是说配置的变量只是针对当前用户生效；下面是系统变量，对于整个系统的所有用户生效。我习惯于配置成系统变量，在此也是用系统变量举例。点击系统变量下的新建按钮，变量名填写 GOROOT,变量值填写上面解压后的go路径，在这也就是 E:\\go，然后点击确定。这个变量是用来配置go的home目录。再点击新建按钮，变量名填写GOPATH，变量值填写E:\\gopath(gopath要存在，当然也可以选择其他文件夹)，点击确定。这个变量是用来指定go查找包的路径，也是用go get 命令所需安装的位置目录。然后找到PATH变量，点击编辑，变量值最后添加一个半角分号，然后再输入 %GOROOT%\\bin，点击确定。可以关闭所有窗口了。在开始菜单中打开cmd命令行（也可以用快捷键，windows键+R，然后输入cmd回车）。在窗口中输入go回车，如果有go相关的帮助打印，则说明配置成功，否则没有成功，重新校验上面的环境变量配置是否正确。 ## Linux 环境 假设对应版本的go解压后绝对路径是/home/user/go。 vi（或者你喜欢的其他编辑器）打开 ~/.bashrc文件，在后面加入 &gt; GOROOT=/home/user/go GOPATH=/home/user/gopath PATH=$PATH:$GOROOT/bin export GOROOT GOPATH PATH 然后 运行命令 `soure ~/.bashrc` 输入 go ，如果打印go帮助文档，说明配置成功，否则检查环境变量配置是否正确。 编写第一个Go程序用你喜欢的编辑器编辑一个文件，加入文件名叫 hello.go 。敲入以下代码1234567package mainimport (\"fmt\")func main()&#123; fmt.println(\"Hello World!\")&#125; 然后再该文件当前目录下，输入 go build hello.go，在目录下会生成一个 hello(windows下是hello.exe)文件，运行该文件，可以看到控制台输入出 HelloWorld,运行成功。","categories":[{"name":"golang","slug":"golang","permalink":"http://yoursite.com/categories/golang/"}],"tags":[{"name":"go","slug":"go","permalink":"http://yoursite.com/tags/go/"},{"name":"golang","slug":"golang","permalink":"http://yoursite.com/tags/golang/"}]},{"title":"删除git已经跟踪的文件或者目录","slug":"git/删除git已经跟踪的文件或者目录","date":"2016-03-13T03:09:29.000Z","updated":"2017-11-04T08:30:45.772Z","comments":true,"path":"2016/03/13/git/删除git已经跟踪的文件或者目录/","link":"","permalink":"http://yoursite.com/2016/03/13/git/删除git已经跟踪的文件或者目录/","excerpt":"","text":"如果第一次提交的时候，没有在gitignore文件中添加忽略文件，那么这些文件（目录也是文件）就会被git跟踪，push的时候也会被推送到远程。所以最好就是一开始在commit之前先添加到gitignore中。 如果文件已经被跟踪且被推送到远程，可以按照下面方法解决：1.rm -rf 文件 git rm -r --cached 要忽略的文件3.git add -A (添加所有)4.git push origin 分支 如果同名的文件过多，如：.class 文件被提交了，那么如果这样一个一个显然效率太低，可以按照下面方法操作 find . -iname 文件名 -exec rm -rf {}\\; 重复上面的步骤，文件名替换为下一个要删除的文件名 修改gitignore，添加忽略文件 git rm -r --cached 要忽略的文件 git add -A git push origin 分支","categories":[{"name":"git","slug":"git","permalink":"http://yoursite.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"hadoop集成hive","slug":"hadoop/hadoop集成hive","date":"2016-03-11T08:25:27.000Z","updated":"2017-11-04T08:30:45.782Z","comments":true,"path":"2016/03/11/hadoop/hadoop集成hive/","link":"","permalink":"http://yoursite.com/2016/03/11/hadoop/hadoop集成hive/","excerpt":"","text":"前置条件hadoop和yarn已经配置好并且成功运行。 选择版本和下载下载hive，有两个版本可以选择，hive1和hive2，hive2版本MR功能已经废弃，将来版本可能会直接去掉，如果要用hive的MR功能，那么请选择hive1相应版本，否则的话选哪个都可以进行测试。 配置hive 下载完hive后，解压，然后sudo vi /etc/profile编辑文件，添加环境变量 1234HIVE_HOME=hive目录的绝路路径PATH=$PATH:$HIVE_HOMEexport HIVE_HOME PATH 控制台输入source /etc/profile使环境变量生效。 进入$HIVE_HOME/conf目录，拷贝hive-default.xml.template并重命名为hive-site.xml，编辑：（如果没有该参数，请自行添加，存在则改之） 1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/user/hive/tmp&lt;/value&gt; &lt;!--hdfs目录 hive目录需要手动创建,并改为777权限--&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;!--hdfs目录，需要手动创建--&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;!--修改为自己的mysql数据库--&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;!--mysql用户名--&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;!--mysql密码--&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt; 然后查找value是${system开头的，替换成具体的本地绝对路径，可以创建一个hive用户，放到hive用户目录下。 拷贝hive-env.sh.template并重命名hive-env.sh，添加如下内容，注意环境变量值更换为自己的路径。 123HADOOP_HOME=/home/hadoop/hadoop-2.7.1 export JAVA_HOME=/usr/local/share/jdk1.8.0_73 将hive-site.xml拷贝一份到hadoop的配置目录。 下载mysql驱动放到hive目录下的lib目录中。 启动hive，控制台输入hive，如果正确则输出一段信息后进入hive交互模式。 问题 Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. 解决方法：下载mysql驱动放到hive的lib目录 org.apache.hadoop.security.AccessControlException: Permission denied: user=hive, access=WRITE, inode=”/tmp/hadoop-yarn/staging/hive/.staging”:hadoop:supergroup:drwxr-xr-x 解决方法：在hdfs中创建tmp目录，并改为777权限。 Starting Job = job_1457683200911_0001, Tracking URL = http://10.10.1.110:8088/proxy/application_1457683200911_0001/Kill Command = /home/hadoop/hadoop-2.7.1/bin/hadoop job -kill job_1457683200911_0001Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0，打开管理页面发现信息：waiting for AM container to be allocated, launched and register with RM. 解决方法：yarn没有配置正确，没有启动nodemanager，启动命令yarn-daemon.sh start nodemanager，启动之后即可运行job。 参考资料 hadoop2.2完全分布式集群+hive+mysql存储元数据配置排版有点乱，请将就看 hive导入HDFS数据 和上面毛病类似","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"},{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"}]},{"title":"SpingXD结合Hadoop","slug":"spring/SpingXD结合Hadoop","date":"2016-03-11T04:34:40.000Z","updated":"2017-11-04T08:30:45.789Z","comments":true,"path":"2016/03/11/spring/SpingXD结合Hadoop/","link":"","permalink":"http://yoursite.com/2016/03/11/spring/SpingXD结合Hadoop/","excerpt":"","text":"前言之前的文章介绍了Spring XD，以分布式方式运行Spring-XD和安装启动Hadoop集群的文章。本文将简单介绍（刚学，很LOW(⊙﹏⊙)b）SpringXD和Hadoop，HDFS结合配置和使用方法。 准备已经按照上述提到的两篇文章或者其他资料搭建并运行了SpringXD和Hadoop。 配置SpringXD 运行命令jps找到AdminServerApplication和ContainerServerApplication两项，结束进程kill 进程pid。 编辑SpingXD的配置文件server.yaml，在spring节点下增加以下hadoop的配置信息： 123456hadoop: &lt;!-- 注意换成自己的hdfs地址 --&gt; fsUri: hdfs://10.10.1.110:8020 resourceManagerHost: 10.10.1.110 resourceManagerPort: 8032 yarnApplicationClasspath: 然后启动xd-adminbin/xd-admin和xd-containerbin/xd-container。 切换到hadoop用户下，使用命令hadoop fs -mkdir /xd创建目录，然后更改权限hadoop fs -chmod -R 777 /xd（如果没有配置hadoop的环境变量，则请进入hadoop的目录使用bin/hadoop命令代替hadoop） 打开新的控制台，进入xd-shell交互环境，假如根据上面的Spring-XD配置文章配置了安全措施，那么还需要执行下面的命令admin config server --uri http://xd-adminIP:9393 --username 用户名 --password 密码进行授权后登录。 创建stream，向hdfs中写入数据stream create --name myhdfsstream1 --definition &quot;time | hdfs&quot; --deploy，用命令hadoop fs ls /xd/myhdfsstream1即可看到有临时文件生成。","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"},{"name":"spring xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"spring-xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"xd","slug":"xd","permalink":"http://yoursite.com/tags/xd/"}]},{"title":"Hexo+Git+Oschina+Golang+Tenxcloud打造博客","slug":"other/Hexo-Git-Oschina-Golang-Tenxcloud打造博客","date":"2016-03-09T15:17:00.000Z","updated":"2017-11-04T08:30:45.787Z","comments":true,"path":"2016/03/09/other/Hexo-Git-Oschina-Golang-Tenxcloud打造博客/","link":"","permalink":"http://yoursite.com/2016/03/09/other/Hexo-Git-Oschina-Golang-Tenxcloud打造博客/","excerpt":"","text":"简介Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。具体使用方法参见这里 Git 介绍和使用参见这里 OSCina 开源信息平台，这里指的是其下的Git托管平台 Golang 谷歌开发的一款跨平台的语言，官方地址在国内无法打开，golangtc是一个Golang学习网站，可以自行查阅。 TenxCloud 也就是时速云，是国内最早的容器云平台之一(Container as a service)，提供丰富的容器化应用，镜像构建与发布，弹性可伸缩的容器服务，以及灵活、高性能的容器主机管理。容器化应用包括但不限于云主机，云数据库，大数据，Web应用等。 准备 安装hexo 安装Golang，并配置Golang环境 安装Git，并配置相关环境变量 创建Git仓库打开开源中国Git托管平台，(注册后)登录，点击右上角+号，新建项目，输入项目名，描述，如果不想公开的话，可以选择私有，其余默认即可，点击创建。然后克隆到本地。命令行切换到刚才克隆的项目根目录，输入hexo init，hexo博客初始化完成。输入hexo generate可以渲染页面，生成静态页面，默认是在public文件夹。hexo默认初始化忽略了public文件夹，我们需要修改.gitignore文件，删除public的记录，这样保证可以同步到git仓库中。 编写Golang Server服务在项目根目录创建一个叫server.go的文件，把下面的代码考入即可。 12345678910111213141516171819202122232425package mainimport ( \"net/http\" \"log\" \"os/exec\")func main() &#123; fs := http.FileServer(http.Dir(\"public\")) http.Handle(\"/\", fs) //用于git的webhook，触发pull http.HandleFunc(\"/_blog/_pull\", func(writer http.ResponseWriter, request *http.Request) &#123; cmd := exec.Command(\"git\",\"pull\") if err:=cmd.Start(); err!=nil &#123; log.Println(\"git pull error\", err) &#125; &#125;) err := http.ListenAndServe(\":80\", nil) if err != nil &#123; log.Fatal(\"ListenAndServe: \", err) &#125;&#125; 通过git提交文件到远程。 创建容器这里之所以选择时速云，是因为一开始接触这类最早的就是这个平台，所以使用的还算熟悉。下面我们就在上面创建一个容器。 登录之后，选择这个镜像，此镜像集成了Git，Golang，SSH服务，点击右侧部署镜像按钮。如下图配置：，点击创建按钮，等待片刻即可创建成功。 返回容器服务，切换到北京2区，可以看到我们刚才创建的容器服务，点击如图所示的图标，进入控制台：进入控制台。 进入控制台后，使用git命令git clone 之前创建的git仓库地址，克隆完后，进入项目目录，输入go build server.go，然后输入./server &amp;运行服务端。 现在打开容器服务视图，找到我们创建的容器，点击右侧的查看所有服务地址，点击协议为HTTP的那个服务地址，在打开的页面中即可看到我们的博客内容。 点击绑定域名，绑定80端口域名，我们就可以通过自己的域名访问了。 设置WebHooks打开开源中国git平台，找到刚创建的项目，点击右侧的管理，然后点击左侧的WebHooks，URL输入http://你的域名/_blog/_pull，Push和合并请求打上勾，然后提交。 说明 不要选择杭州区的服务，因为采用的是阿里云服务，所以会导致没有备案的域名没法打开。 本博客公开托管在开源中国的Git服务上，大家可以fork之后改动后用作自己的博客平台。 方便起见，Golang脚本已经编译成了Windows Mac Linux 各平台的32和64位版本，无需编译server.go文件了，可以直接选择相应平台文件进行运行，启动web服务。","categories":[{"name":"other","slug":"other","permalink":"http://yoursite.com/categories/other/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]},{"title":"安装启动Hadoop集群","slug":"hadoop/安装启动Hadoop集群","date":"2016-03-09T09:49:43.000Z","updated":"2017-11-04T08:30:45.784Z","comments":true,"path":"2016/03/09/hadoop/安装启动Hadoop集群/","link":"","permalink":"http://yoursite.com/2016/03/09/hadoop/安装启动Hadoop集群/","excerpt":"","text":"环境UCloud云主机，2.6.32-431.11.15.el6.ucloud.x86_64假设三台主机内网IP分别为master, 10.10.1.11和10.10.1.12，hostname分别为：10-10-1-10,10-10-1-11和10-10-1-12 配置JDK本次搭建测试用的是jdk8，可以从Oracle官网下载对应的版本。 配置jdk假设jdk解压后目录存放在/usr/local/jdk8，命令行输入sudo vi /etc/profile，添加一下内容：123JAVA_HOME=/usr/local/jdk8CLASSPATH=.:$JAVA_HOME/jre/libPATH=$PATH:$JAVA_HOME/bin 然后输入source /etc/profile使环境变量生效，输入java -version有java版本信息输出说明配置成功，三台主机均这么配置。 配置hadoop用户控制台输入sudo useradd -m -U hadoop 添加hadoop用户，然后输入sudo passwd hadoop 修改hadoop用户的密码，输入su -l hadoop，输入刚才设置的密码，切换到hadoop用户。 配置ssh前提是已经切换到hadoop用户。在每个主机控制台输入ssh-keygen回车，一直回车直到结束。最后在master主机上使用ssh-copy-id命令拷贝认证信息到本主机和其他两台主机，这样可以免密码登录。ssh-copy-id hadoop@主机地址， 配置网络三台主机均在 /etc/hosts 添加以下映射:123master master10.10.1.11 slave110.10.1.12 slave2 配置Hadoop 下载Hadoop，这里下载的是2.7.1版本。控制输入wget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz，等待下载完成。 控制台输入tar -xvf hadoop-2.7.1.tar.gz解压，会生成hadoop-2.7.1目录。 输入cd hadoop-2.7.1/etc/hadoop进入配置文件目录。 修改hadoop-env.sh中的export JAVA_HOME=，将等号后的内容改成上面配置的jdk绝对路径，在这里就是/usr/local/jdk8，修改完后应该是export JAVA_HOME=/usr/local/jdk8，保存退出。 修改core-site.xml，配置config内容： 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- 注意，这里改成自己本机的ip --&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.namenode&lt;/name&gt; &lt;!-- 注意，这里改成自己本机的ip --&gt; &lt;value&gt;hdfs://master:8082&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.native.lib&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Should native hadoop libraries, if present, be used.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml，修改config内容为： 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;!-- 注意修改为自己的ip --&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;!-- 注意ip改为自己的 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;master:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改slaves文件，添加其他两台ip 12slave1slave2 将hadoop目录覆盖到其余机器对应目录。下面开始操作hadoop命令，如果遇到hadoop native错误，请查看文末Hadoop Native 配置部分。 格式化文件系统注意：这里的格式化文件系统并不是硬盘格式化，只是针对主服务器hdfs-site.xml的dfs.namenode.name.dir和dfs.datanode.data.dir目录做相应的清理工作。切换到Hadoop的home目录，执行bin/hdfs namenode -format。 启动停止服务启动sbin/start-dfs.sh，可以一次性启动master和slaves节点服务。sbin/start-yarn.sh启动yarn资源管理服务。要停止服务，用对应的sbin/stop-dfs.sh和sbin/stop-dfs.sh即可停止服务。 单独启动一个datanode增加节点或者重启节点，需要单独启动，则可使用以下命令:sbin/hadoop-daemon.sh start datanode，启动nodeManagersbin/yarn-daemon.sh start nodemanager，当然也可以操作namenodesbin/hadoop-daemon.sh start namenode sbin/yarn-daemon.sh start resourcemanager。注意：原文中是sbin/yarn-daemons.sh和sbin/hadoop-daemons.sh，运行后发现并没有启动成功，去掉s后启动成功。 Hadoop Native 配置输入 hadoop checknative 检查Hadoop本地库版本和相关依赖信息： 1234567891011121316/03/10 12:17:56 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...16/03/10 12:17:56 DEBUG util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: /home/hadoop/hadoop-2.6.3/lib/native/libhadoop.so.1.0.0: /lib64/libc.so.6: version `GLIBC_2.14&apos; not found (required by /home/hadoop/hadoop-2.6.3/lib/native/libhadoop.so.1.0.0)16/03/10 12:17:56 DEBUG util.NativeCodeLoader: java.library.path=/home/hadoop/hadoop-2.6.3/lib/native16/03/10 12:17:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable16/03/10 12:17:56 DEBUG util.Shell: setsid exited with exit code 0Native library checking:hadoop: falsezlib: falsesnappy: falselz4: falsebzip2: falseopenssl: false16/03/10 12:17:56 INFO util.ExitUtil: Exiting with status 1 发现/lib64/libc.so.6: versionGLIBC_2.14’ not found`信息，说明该版本的Hadoop需要glibc_2.14版本。下面就安装所需的版本。 mkdir glib_build &amp;&amp; cd glib_build wget http://ftp.gnu.org/gnu/glibc/glibc-2.14.tar.gz &amp;&amp; wget http://ftp.gnu.org/gnu/glibc/glibc-linuxthreads-2.5.tar.bz2 tar zxf glibc-2.14.tar.gz &amp;&amp; cd glibc-2.14 &amp;&amp; tar jxf ../glibc-linuxthreads-2.5.tar.bz2 cd ../ &amp;&amp; export CFLAGS=&quot;-g -O2&quot; &amp;&amp; ./glibc-2.14/configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin make make installinstall最后会遇到错误信息： 12345678910111213141516171819CC=&quot;gcc -B/usr/bin/&quot; /usr/bin/perl scripts/test-installation.pl /root//usr/bin/ld: cannot find -lnss_test1collect2: ld returned 1 exit statusExecution of gcc -B/usr/bin/ failed!The script has found some problems with your installation!Please read the FAQ and the README file and check the following:- Did you change the gcc specs file (necessary after upgrading from Linux libc5)?- Are there any symbolic links of the form libXXX.so to old libraries? Links like libm.so -&gt; libm.so.5 (where libm.so.5 is an old library) are wrong, libm.so should point to the newly installed glibc file - and there should be only one such link (check e.g. /lib and /usr/lib)You should restart this script from your build directory after you&apos;vefixed all problems!Btw. the script doesn&apos;t work if you&apos;re installing GNU libc not as yourprimary library!make[1]: *** [install] Error 1make[1]: Leaving directory `/root/glibc-2.14&apos;make: *** [install] Error 2 无需关注，检验是否成功ls -l /lib64/libc.so.6lrwxrwxrwx 1 root root 12 Mar 10 12:12 /lib64/libc.so.6 -&gt; libc-2.14.so出现了/lib64/libc.so.6 -&gt; libc-2.14.so字样说明成功了。 安装opensslyum install openssl-static.x86_64 如何修改主机名称修改文件/etc/sysconfig/network然后执行/etc/rc.d/init.d/network restart重启网络模块 secondaryNameNode 配置 修改masters文件（如果没有则自己创建），添加一个主机名称，用以作为secondaryNameNode。 修改hdfs-site.xml的内容，删除dfs.namenode.secondary.http-address部分配置，添加新的配置（注意修改为自己的ip）： 123456789101112&lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;description&gt; The address and the base port where the dfs namenode web ui will listen on. If the port is 0 then the server will start on a free port. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;10.10.1.11&lt;/value&gt;&lt;/property&gt; 参考资料： Hadoop-2.5.2集群安装配置详解 基于hadoop2.2的namenode与SecondaryNameNode分开配置在不同的计算机","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"}]},{"title":"Spring-XD简介","slug":"spring/Spring-XD简介","date":"2016-03-09T07:30:24.000Z","updated":"2017-11-04T08:30:45.789Z","comments":true,"path":"2016/03/09/spring/Spring-XD简介/","link":"","permalink":"http://yoursite.com/2016/03/09/spring/Spring-XD简介/","excerpt":"","text":"简介Spring XD is a unified, distributed, and extensible service for data ingestion, real time analytics, batch processing, and data export. Streams翻译过来就是流，通过定义stream可以控制数据的流向，比如从MongoDB读取数据然后存储到HDFS中。 创建方式一个简单的示例：该示例创建一个名字叫ticktock的stream，每秒钟产生一条时间信息然后通过管道传送到log中。1xd:&gt; stream create --definition &quot;time | log&quot; --name ticktock 销毁Stream1xd:&gt; stream destroy --name stream-name Modules模块，当前包含source, sink,processor, 和job。 Souces数据源，Stream的来源，有以下几种方式： 方式 描述 File 文件方式 FTP FTP方式 GemFire Continuous Query(gemfire-cq) GemFire查询 GemFire source(gemfire) GemFire文件 HTTP http方式 JDBC Source(jdbc) 关系型数据库jdbc JMS JMS Kafka Kafka消息队列 Mail 通过接收电子邮件 MongoDB Source(mongodb) MongoDB数据库 MQTT MQTT RabbitMQ RabbitMQ消息队列 Reactor IP(reactor-ip) Reactor IP(reactor-ip) SFTP SFTP Stdout Capture 标准输入 Syslog 系统日志 Tail Tail程序 TCP TCP TCP Client(tcp-client) TCP 客户端 Time 时间 Trigger Source(trigger) 触发器 Twitter Search(twittersearch) Twitter搜索 Twitter Stream(twitterstream) Twitter Stream流 Sinks数据源，Stream的输出，有以下几种方式： 方式 描述 Dynamic Router(router) 动态路由 File Sink(file) 文件方式 FTP Sink(ftp) FTP方式 GemFire Server GemFire服务器 GPFDIST GPFDIST Cassandra Cassandra 数据库 Hadoop(HDFS) (hdfs) hdfs文件系统 HDFS Dataset(Avro/Parquet) (hdfs-dataset) hdfs文件系统中的avro或者parquet类型文件 JDBC Source(jdbc) 关系型数据库jdbc Kafka Sink (kafka) Kafka消息队列 Log log 文件 Mail Mail 发送 Mongo Mongo数据库 MQTT Sink (mqtt) MQTT Null Sink(null) null RabbitMQ RabbitMQ 消息队列 Redis Redis Shell Sink (shell) shell Splunk Server (splunk) splunk TCP Sink (tcp) TCP Processors可用的处理器包括Aggregator Filter Header Enricher HTTP Client JSON to Tuple Object to JSON Script Shell Command Splitter Transform Aggregator – 作用和 splitter相反，用于聚合， Splitter – 用于拆解 Filter – 过滤器，用于中间处理数据 Header Enricher (header-enricher) – 用于添加头部信息 HTTP Client – 通过httpClient方式发送URL请求 JSON to Tuple (json-to-tuple) – 转换json数据到Tuple类型 Object to JSON (object-to-json) – 将对象转换为json格式 Script 用于加载Groovy脚本 Shell – 用于加载Shell脚本 Transform – 用于负载类型转换 Taps监听器，窃听器。不用重复定义相同的stream，然后监听此stream就可以做其他操作。并且可以用Label来分别对每个部分内容做个别名，定义Tab时候可以使用别名。如12stream create foo --definition &quot;httpLabel: http | fLabel: filter --expression=payload.startsWith(&apos;A&apos;) | flibble: transform --expression=payload.toLowerCase() | log&quot; --deploystream create fooTap --definition &quot;tap:stream:foo.flibble &gt; log&quot; --deploy 上面对trasfrom部分做了一个别名，叫做flibble，然后下面定义一个Tap，并且最后指定是flibble这个标签，那么就是对foo这个stream的flibble做监听。 JobsJob相比Stream不同点在于，Job算是静态的，Stream是动态的。Stream会持续接收数据，处理数据；Job是一次性接收数据，处理数据，如果数据改变，那么是不会进行处理的，除非有定时任务。12job create --name jobtest --definition &apos;timestampfile --directory=D:/jobs&apos; --deploystream create --name time-cron --definition &quot;trigger --cron=&apos;* * * * * *&apos; &gt; queue:job:jobtest&quot; --deploy 使用counter123456stream create foo --definition &apos;http --outputType=application/json | log&apos;stream create countName --definition &apos;tap:stream:foo &gt; field-value-counter --fieldName=name&apos; --deploystream deploy --name foohttp post --data &#123;&quot;name&quot;:&quot;a&quot;&#125;http post --data &#123;&quot;name&quot;:&quot;a&quot;&#125;http post --data &#123;&quot;name&quot;:&quot;b&quot;&#125; field-value-counter list 列出field-value-counter的名字123FieldValueCounter name----------------------countName field-value-counter display --name countName 列出名字为countName的描述 FieldValueCounter=countName --------------------------- - ----- VALUE - COUNT a | 2 b | 1","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"spring xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"spring-xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"xd","slug":"xd","permalink":"http://yoursite.com/tags/xd/"}]},{"title":"以分布式方式运行Spring-XD","slug":"spring/以分布式方式运行Spring-XD","date":"2016-03-09T06:18:33.000Z","updated":"2017-11-04T08:30:45.790Z","comments":true,"path":"2016/03/09/spring/以分布式方式运行Spring-XD/","link":"","permalink":"http://yoursite.com/2016/03/09/spring/以分布式方式运行Spring-XD/","excerpt":"","text":"主要以官方文档说明进行配置 简介Spring XD分布式运行环境（DIRT）支持以分布式方式运行多个跨节点的任务。参见Getting Started获取以单个节点运行方式的信息。 XD的分布式运行架构主要由以下组件构成： Admin 主要用于管理Stream，Job的发布，用户操作，和提供运行时相关的状态，系统统计和分析的REST服务 Container 托管发布的模块（Stream处理任务）和批量任务 ZooKeeper 提供所有XD运行时的信息。追踪Container信息，如：modules，jobs发布情况，steam定义，发布状态等。 Spring Batch Job Repository Database –这个要求要配置一个关系型数据库。XD包含了HSQLDB，但是不推荐用在生产环境中。XD支持任何JDBC型数据库。 A Message Broker –用于数据传输。XD的数据传输模块设计成了插拔式。当前XD版本支持Rabbit MQ和Redis，这两个都支持stream和job过程产生的数据的传输，Kafka仅支持steam产生的数据传输。请注意：job使用Kafka作为数据传输是不稳定的。这个项目必须要配置一个作为数据传输的插件（推荐Redis）。 Analytics Repository – XD目前用Redis作为counters和gauges分析的存储方式。XD的分布式运行环境概览如下： Server Configuration默认查找$XD_HOME/config/servers.yml文件，作为配置文件。但是可以使用XD_CONFIG_LOCATION环境变量改变配置文件夹，使用XD_CONFIG_NAME改变配置文件位置，如：12export XD_CONFIG_LOCATION=file:/xd/config/export XD_CONFIG_NAME=region1-servers 注意，XD_CONFIG_LOCATION最后的/是必须的。 Database ConfigurationMySQL，PostGresql选其中一个配置即可，当然还有Oracle也可以配置，但是在这里没有列出，可以]参考官方文档xd-singlenode模式是使用了一个嵌入式HSQLDB数据库，运行分布式模式的时候，可以使用独立的HSQLDB，但是仅仅推荐在学习和开发的时候使用它，正式环境最好使用其他比如MySQL，Postgres等等数据库。 注意：如果在stream模块中使用除了Postgres和HSQLDB数据库，那么需要把对应的驱动放到$XD_HOME/lib目录。servers.yml文件中已经注释了一部分jdbc配置信息，可以按需更改。 MySQL配置123456spring: datasource: url: jdbc:mysql://yourDBhost:3306/yourDB username: yourUsername password: yourPassword driverClassName: com.mysql.jdbc.Driver Postgresql配置123456spring: datasource: url: jdbc:postgresql://yourDBhost:5432/yourDB username: yourUsername password: yourPassword driverClassName: org.postgresql.Driver Redisstream和job的数据传输需要用到（Rabbit MQ也可以）（当用作数据分析时候也需要），这里推荐统一使用redis作为配置，123456789spring: redis: port: 6379 host: localhost pool: maxIdle: 8 # max idle connections in the pool minIdle: 0 # min idle connections in the pool maxActive: -1 # no limit to the number of active connections maxWait: 30000 # time limit to get a connection - only applies if maxActive is finite 安装redis从官网下载最新的redis，然后解压，进入redis根目录，1234cd depsmake hiredis jemalloc linenoise luacd ..make install 注意：依赖于gcc和make（Ubuntu系列如果没有安装 apt-get install gcc make） 开启页面登录密码保护默认ui管理界面是没有安全配置的，不需密码即可访问，为了安全起见，我们可以设置登录用户和密码。12345678910spring: profiles: admin (1)security: basic: enabled: true (2) realm: SpringXD user: name: yourAdminUsername password: yourAdminPassword role: ADMIN, VIEW, CREATE 注意：spring.batch.initializer.enabled默认是true，会使Spring Bath初始化表结构。 启动adminadmin只会有一个，用来协调container和管理相关stream，job等。xd/bin/xd-admin 启动containercontainer可以启动多个，也就是组成多个节点，由此构成分布式运行环境。1xd/bin/xd-container 创建Stream进入$XD_HOME/shell/,控制台输入bin/xd-shell，进去xd命令行交互模式，然后输入1stream create --name foo --definition &apos;time | log&apos; --deploy 即可看见admin日志（控制台没关闭的话也可以看到）有时间信息输出。 下面这个stream是从kafka读取信息，然后传输到log里，所以需要配置kafka，请查阅kafka相关资料。 1stream create --name kafkaDevice --definition &apos;kafka --outputType=text/plain --zkconnect=10.10.1.20:2181 --topic=kafka_test --offsetStorage=redis | log &apos; --deploy 末最后贴出一份比较完整的配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647security: basic: enabled: true realm: SpringXD user: name: hekr password: hekr # 必须配置角色才会生效 role: ADMIN, VIEW, CREATEspring: redis: port: 6379 host: 127.0.0.1 pool: maxIdle: 8 # max idle connections in the pool minIdle: 0 # min idle connections in the pool maxActive: -1 # no limit to the number of active connections maxWait: 30000 # time limit to get a connection - only applies if maxActive is finite #sentinel: # master: mymaster # nodes: 127.0.0.1:26379,127.0.0.1:26380,127.0.0.1:26381 batch: isolationLevel: ISOLATION_SERIALIZABLE # clobType: dbType: MYSQL maxVarcharLength: 2500 tablePrefix: BATCH_ validateTransactionState: true initializer: enabled: true datasource: url: jdbc:mysql://localhost:3306/xd username: root password: hekr driverClassName: com.mysql.jdbc.Driver testOnBorrow: true validationQuery: select 1zk: namespace: xd client: connect: 10.10.1.20:2181 sessionTimeout: 60000 connectionTimeout: 30000 initialRetryWait: 1000 retryMaxAttempts: 3xd: transport: redis ##注意： 配置权限后，进入xd-shell会显示server-unknown:&gt;， 需要配置一下admin server才能进入交互 1admin config server --uri http://服务器地址:端口(默认9393) --username 用户名 --password 密码 testOnBorrow默认是true，如果配置为true或者没有配置，则需要配置正确的validationQuery，如果配置不正确则会有类似如下异常出现1Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not get JDBC Connection; nested exception is java.sql.SQLException: Failed to validate a newly established connection.","categories":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/categories/spring/"}],"tags":[{"name":"spring xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"spring-xd","slug":"spring-xd","permalink":"http://yoursite.com/tags/spring-xd/"},{"name":"xd","slug":"xd","permalink":"http://yoursite.com/tags/xd/"}]},{"title":"Jrebel插件配置参数","slug":"java/Jrebel插件配置参数","date":"2016-03-09T06:10:18.000Z","updated":"2017-11-04T08:30:45.784Z","comments":true,"path":"2016/03/09/java/Jrebel插件配置参数/","link":"","permalink":"http://yoursite.com/2016/03/09/java/Jrebel插件配置参数/","excerpt":"","text":"JVM 参数-javaagent:/path/jrebel.jar SpringSpring Bean/Core/MVC/Security/Webflow/WS-Drebel.spring_plugin=true Spring Data-Drebel.spring_data_plugin=true Struts-Drebel.struts2_plugin=true Hibernate-Drebel.hibernate_plugin=true Hibernate Validator-Drebel.hibernate_validator_plugin=true MyBatis-Drebel.mybatis_plugin=true Logback-Drebel.logback_plugin=true Log4J 2-Drebel.log4j2_plugin=true #Groovy-Drebel.groovy_plugin=true Jruby-Drebel.jruby_plugin=true GWT-Drebel.gwt_plugin=true 参考plugins","categories":[{"name":"java","slug":"java","permalink":"http://yoursite.com/categories/java/"}],"tags":[{"name":"jrebel","slug":"jrebel","permalink":"http://yoursite.com/tags/jrebel/"},{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"archlinux安装ntfs驱动","slug":"linux/archlinux安装ntfs驱动","date":"2016-03-09T06:08:56.000Z","updated":"2017-11-04T08:30:45.786Z","comments":true,"path":"2016/03/09/linux/archlinux安装ntfs驱动/","link":"","permalink":"http://yoursite.com/2016/03/09/linux/archlinux安装ntfs驱动/","excerpt":"","text":"默认情况下，archlinux本身支持挂在ntfs文件系统，只不过是只读，不能写入。如果要支持ntfs系统文件，那么需要安装ntfs的驱动程序。用命令 yaourt -Ss ntfs 可以查找关于ntfs的软件包&gt;extra/ntfs-3g 2014.2.15-1 [installed] NTFS filesystem driver and utilitiesaur/disk-manager 1.0.1-3 (56) A tool to manage filesystems, partitions, and NTFS write modeaur/fgetty 0.7-5 (14) A mingetty stripped of the printfsaur/fgetty-pam 0.7-4 (4) A mingetty stripped of the printfs, patched for PAM-support.aur/grub4dos 0.4.5c_20140822-1 (35) A GRUB boot loader support menu on windows(fat,ntfs)/linux(ext2,3,4)aur/libntfs-wii 2013.1.13-1 (1) NTFS-3G filesystem access library (for Nintendo Gamecube/Wii homebrew development)aur/ntfs-3g-ar 2014.2.15AR.3-1 (37) NTFS filesystem driver and utilities with experimental featuresaur/ntfs-3g-fuse 2014.2.15-1 (46) Stable read and write NTFS driver and ntfsprogs. This package will allow normal users to mount NTFS Volumes.aur/ntfs-3g_ntfsprogs-git 4695.db35a16-1 (7) Read and write NTFS driver and utilities - GIT versionaur/ntfs-config 1.0.1-13 (119) Enable/disable NTFS write support with a simple clickaur/ntfsfixboot 1.0-3 (18) Fix NTFS boot sectoraur/scrounge-ntfs 0.9-2 (28) Data recovery program for NTFS file systemsaur/ufsd-module 8.9.0-3 (9) Paragon NTFS &amp; HFS for Linux driver. - ACLs removedaur/ufsd-module-dkms 8.9.0-3 (4) Paragon NTFS &amp; HFS for Linux driver. - ACLs removed. DKMS versionaur/wipefreespace 2.0-1 (3) Securely wipe the free space on an ext2/3/4,NTFS, XFS,ReiserFSv3, ReiserFSv4, FAT12/16/32,Minix,JFS and HFS+ partition or drive 可以看到有这么多相关软件包，其实只需要安装第一个也就是 extra/ntfs-3g 就可以了，输入命令 yaourt -S extra/ntfs-3g，安装完毕后重新插入ntfs分区U盘或者移动硬盘就可以进行写入操作了。","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"arch","slug":"arch","permalink":"http://yoursite.com/tags/arch/"},{"name":"archlinux","slug":"archlinux","permalink":"http://yoursite.com/tags/archlinux/"},{"name":"ntfs","slug":"ntfs","permalink":"http://yoursite.com/tags/ntfs/"}]},{"title":"archlinux安装","slug":"linux/archlinux安装","date":"2016-03-09T06:05:03.000Z","updated":"2017-11-04T08:30:45.786Z","comments":true,"path":"2016/03/09/linux/archlinux安装/","link":"","permalink":"http://yoursite.com/2016/03/09/linux/archlinux安装/","excerpt":"","text":"前言本人也是第一次安装archlinux，严格来说是第一次安装成功，记录一下，既为自己也为新手。此方式是非UEFI模式，并且分区表使DOS的MBR方式，GPT分区表没有测试。以后也许会在虚拟机中测试过进行补充。 温馨提示建议现在虚拟机中安装几次，直到安装成功，并且可以正常开机，上网，打开桌面环境，有十足把握之后再在物理机上安装，以免中间出现问题又没办法解决。并在虚拟机安装的过程中记录遇到的问题，以便日后参考。同时安装的时候记得备份重要文件，以免安装错误导致文件丢失。 准备安装介质 首先准备archlinux镜像，如果没有可以点击这里下载，最好选择中国的镜像服务，比如网易的。下载完成后校验一下MD5值（官方文件的MD5值在md5sums.txt 这个文件中），如果相同那么可以进行下一步了；如果不相同需要重新下载并校验，不推荐在MD5值不同的情况下继续进行，因为不知道会发生什么问题。 刻录至U盘。如果用的是linux系统或者Mac系统（话说这么优雅的系统为啥要换呢，也可能是双系统吧），可以使用 dd 命令。把U盘插入计算机， 输入命令 ls -al /dev/sd*, 一般sdb是你的U盘，也请先做好文件备份。 现在假定U盘是 /dev/sdb, archlinux的文件路径是 /home/user/archlinux.iso,那么输入命令(需root权限) dd -if=/home/user/archlinux.iso -of=/dev/sdb，然后等待命令执行完毕，如果没有任何提示，则代表成功了。 环境准备 再次提醒，做好文件备份 如果没有分区的话，先进行分区，并进行格式化，如果已经操作过了或者想重用上次系统(Linux)的分区，可以直接进入第2步。 分区进行时：敲入命令 fdisk /dev/sda (假定操作的磁盘时sda，请自行确认好，此操作要格外小心)。输入 m 可以查看帮助， n 是新建一个分区， d 是删除一个已有分区。如果想新建一个DOS分区表，则输入 o，已经有分区表，想重新分区的话，按 d ，直到删除所有分区。分区方案可以按照以下来： /boot 大概需要 200M， / 可以分配 15G ～ 40G， /var 8G～ 20G (可选) ， /tmp 4G ～ 8G (可选)，其余分给 /home分区（强烈建议单独分区，以后重装系统可以不用拷贝主目录下的资料了）， swap 4G ～ 8G（可选）。新建分区输入 n，默认（p主分区）即可，然后默认（分区号1），接下来也是默认扇区既可以，然后选择大小，可以输入G,M,K单位的大小，我们输入 +200M，确定；然后创建根分区，按n,一路下来，大小选择输入 +15G,确定，根分区创建完成。如果分区少于4个，可以按照上面步骤，直到分区创建完成；但是如果分区多于4个，就要创建扩展分区，然后再创建逻辑分区了。扩展分区的创建和上面一样，只不过在选择分区格式的时候不是输入 p 了，而是 e，其余一样的。创建逻辑分区的时候输入 l （英文L的小写字母），剩下的步骤也是和创建主分区一样的啦。所有分区创建完成后，输入 w ，上面的一系列操作才会真正写入磁盘，再次之前都是在内存中，所以，在按 w 之前，还是有后悔药吃的，但是按下之后，那就定格了。切记！ 格式化分区：格式化分区的命令是 mkfs.xxx，输入 mkfs.，按 Tab 键可以看到有如下格式： mkfs.bfs mkfs.ext2 mkfs.ext4 mkfs.jfs mkfs.reiserfs mkfs.cramfs mkfs.ext3 mkfs.ext4dev mkfs.minix mkfs.xfs。咦，好像没有swap分区格式，swap分区格式化的命令是 mkswap 啦。输入命令 mkfs.ext4 /dev/sda1 将 /boot 分区格式化成ext4格式的分区，根分区和其他非swap分区用此方法依次格式化，用 mkswap /dev/sdax 格式化上面分的swap分区，x是分swap分区所得的号码。 安装 mount 相关分区。 mkdir /mnt/home /mnt/tmp /mnt/var /mnt/boot 创建home，tmp，var，boot挂载点目录，然后 mount /dev/sdax /mnt/xx x代表分区号，xx代表目录，把的分区挂载到相应挂载点上。 修改 /etc/pacman.d/mirrorlist 的镜像列表，可以删除所有的，然后输入&gt;Server = http://mirrors.163.com/archlinux/$repo/os/x86_64 保存退出。 执行 pacstrap /mnt base 命令进行基础安装。 生成fstab。 genfstab -p /mnt &gt;&gt; /mnt/etc/fstab, 查看一下/mnt/etc/fstab 内容格式是否正确，有无重复内容，如有请先订正。格式大体如下：&gt;## /etc/fstab: static file system information## # UUID=ee8bae58-9428-4917-b63e-0258d19a4567/dev/sda5 / ext4 rw,relatime,data=ordered0 1# UUID=cbac48fe-3345-4cba-96ec-acdbdc56d0ad/dev/sda9 /home ext4 rw,relatime,data=ordered0 2# UUID=59e210c2-fced-4cdd-b631-d9a50ba82312/dev/sda7 /tmp ext4 rw,relatime,data=ordered0 2 切换到新系统的root目录下，命令 arch-choot /mnt 设置主机名 echo your_hostname &gt; /etc/hostname ， your_hostname换成你想要的，最好是纯英文。 设置时区。 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime。 修改 /etc/locale.gen ， 添加以下内容&gt;en_US.UTF-8 UTF-8zh_CN.UTF-8 UTF-8 执行 locale-gen， 执行 echo LANG=zh_CN.UTF-8 &gt; /etc/locale.conf 设置键盘映射和字体，文件在 /etc/vconsole.conf，在这就保持默认配置了。 设置root密码 passwd 然后输入密码，再输入一次确认。 安装引导程序，这里用grub。 pacman -Sy grub，安装完成后，执行 pacman-db-upgrade， 然后再执行 grub-install --target=i386-pc --recheck --debug /dev/sda 安装grub引导到sda上。最后执行 grub-mkconfig -o /boot/grub/grub.cfg ，生成引导配置。 重启， 执行 reboot。如果成功安装的话，会出现grub引导选择系统菜单，选择默认的进入，输入root用户名，输入密码，登录成功。至此，安装已经完成，接下来是配置。 配置 网络配置 查看网络设备名称 ls /sys/class/net, 记住所看到的网卡接口名称，假定叫做eth0 启用网络接口 ip link set eth0 up 检查结果状态 ip link show dev eth0 如果打印&gt;enp3s0: mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000link/ether 00:e0:66:cb:e2:1e brd ff:ff:ff:ff:ff:ff 类似内容，说明启用成功。 创建或编辑 /etc/systemd/network/dhcp.network ,添加以下内容：&gt;[Match]Name=en*[Network]DHCP=v4 启用网络服务 systemctl enable systemd-resolved 编辑 /etc/resolv.conf 配置dns ， 添加以下内容：&gt;nameserver 8.8.8.8nameserver 4.4.4.4 如果你的IP段在192.168.xxx.yyy,则再添加 nameserver 192.168.xxx.1 执行 dhcpd 启用dhcp，要开机自动启动dhcp服务，则执行 systemctl enable dhcpd基本环境配置已经完成。 桌面环境配置安装 fxce4 pacman -S xorg xorg-serverpacman -S slim #登录管理器pacman -S xfce4pacman -S xfce4-goodiespacman -S fortune-modepacman -S gamin 创建用户 useradd -Um du 设置密码 passwd du 切换用户 su -l du 输入 startxfce4 可以进入xfce桌面了 美化显示： 字体 首先可以从windowns上或者其他地方准备字体文件，然后 cp *.ttf ~/.fonts/ 建立字体缓存mkfontscalemkfontdirfc-cache -fv 输入法 传送门输入法就安装fcitx小企鹅输入法了 安装输入法 pacman -S fcitx 配置输入法 安装输入法其他模块 fcitx-ui-light Fcitx 的轻量 UI.fcitx-fbterm Fbterm 对 Fcitx 的支持。fcitx-table-extra Fcitx 的一些额外码表支持，包括仓颉 3, 仓颉 5, 粤拼, 速成, 五笔, 郑码等等fcitx-table-other Fcitx 的一些更奇怪的码表支持，包括 Latex, Emoji, 以及一大堆不明字符等等。kcm-fcitx KDE 的 Fcitx 输入法模块 启动桌面环境时候启用输入法 在 .bashrc 文件中加入如下代码 &gt;export GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS=”@im=fcitx” 退出用户，重新登陆，可以欢快的使用输入法了。 ps:浙大源：Server = http://mirrors.zju.edu.cn/archlinux/$repo/os/$arch网易源：Server = http://mirrors.163.com/archlinux/$repo/os/x86_64北京交大：Server = http://mirror.bjtu.edu.cn/ArchLinux/$repo/os/x86_64","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"arch","slug":"arch","permalink":"http://yoursite.com/tags/arch/"},{"name":"archlinux","slug":"archlinux","permalink":"http://yoursite.com/tags/archlinux/"}]},{"title":"sql查找排除某些表后不存在某个字段的表","slug":"sql/sql查找排除某些表后不存在某个字段的表","date":"2016-03-09T06:03:56.000Z","updated":"2017-11-04T08:30:45.791Z","comments":true,"path":"2016/03/09/sql/sql查找排除某些表后不存在某个字段的表/","link":"","permalink":"http://yoursite.com/2016/03/09/sql/sql查找排除某些表后不存在某个字段的表/","excerpt":"","text":"12345678SELECT table_name FROM information_schema.tablesWHERE table_schema='database_name' AND table_name NOT LIKE 'table_name' AND table_name NOT IN(SELECT col.table_name FROM information_schema.`COLUMNS` col WHERE col.table_name IN (SELECT table_name FROM information_schema.tables WHERE table_schema='database_name' AND table_name NOT LIKE 'table_name' AND col.column_name='gmt_modified');","categories":[{"name":"sql","slug":"sql","permalink":"http://yoursite.com/categories/sql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"},{"name":"sql","slug":"sql","permalink":"http://yoursite.com/tags/sql/"}]},{"title":"MySql避免重复插入记录","slug":"sql/MySql避免重复插入记录","date":"2016-03-09T06:02:47.000Z","updated":"2017-11-04T08:30:45.791Z","comments":true,"path":"2016/03/09/sql/MySql避免重复插入记录/","link":"","permalink":"http://yoursite.com/2016/03/09/sql/MySql避免重复插入记录/","excerpt":"","text":"方案一：使用ignore关键字如果是用主键primary或者唯一索引unique区分了记录的唯一性,避免重复插入记录可以使用：1insert ignore into table_name(email,phone,user_id) values('test9@163.com','99999','9999') 这样当有重复记录就会忽略,执行后返回数字0,还有个应用就是复制表,避免重复记录：1insert ignore into table(name) select name from table2 方案二：使用Replacereplace的语法格式为： replace into table_name(col_name, ...) values(...) replace into table_name(col_name, ...) select ... replace into table_name set col_name=value, ... 算法说明：REPLACE的运行与INSERT很相像,但是如果旧记录与新记录有相同的值，则在新记录被插入之前，旧记录被删除，即： 尝试把新行插入到表中 当因为对于主键或唯一关键字出现重复关键字错误而造成插入失败时： 从表中删除含有重复关键字值的冲突行 再次尝试把新行插入到表中旧记录与新记录有相同的值的判断标准就是：表有一个PRIMARY KEY或UNIQUE索引，否则，使用一个REPLACE语句没有意义。该语句会与INSERT相同，因为没有索引被用于确定是否新行复制了其它的行。返回值：REPLACE语句会返回一个数，来指示受影响的行的数目。该数是被删除和被插入的行数的和。受影响的行数可以容易地确定是否REPLACE只添加了一行，或者是否REPLACE也替换了其它行：检查该数是否为1（添加）或更大（替换）。示例:eg:(phone字段为唯一索引)replace into table_name(email,phone,user_id) values(&#39;test569&#39;,&#39;99999&#39;,&#39;123&#39;)另外：在 SQL Server 中可以这样处理：1234if not exists (select phone from t where phone= '1') insert into t(phone, update_time) values('1', getdate())else update t set update_time = getdate() where phone= '1' 更多信息请看 方案三：ON DUPLICATE KEY UPDATE如‍上所写，你也可以在INSERT INTO.....后面加上 ON DUPLICATE KEY UPDATE方法来实现。如果您指定了ON DUPLICATE KEY UPDATE，并且插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行旧行UPDATE。例如，如果列a被定义为UNIQUE，并且包含值1，则以下两个语句具有相同的效果：12INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1; `UPDATE table SET c=c+1 WHERE a=1; 如果行作为新记录被插入，则受影响行的值为1；如果原有的记录被更新，则受影响行的值为2。注释：如果列b也是唯一列，则INSERT与此UPDATE语句相当：12UPDATE table SET c=c+1WHERE a=1 OR b=2 LIMIT 1; 如果a=1 OR b=2与多个行向匹配，则只有一个行被更新。通常，您应该尽量避免对带有多个唯一关键字的表使用ON DUPLICATE KEY子句。您可以在UPDATE子句中使用VALUES(col_name)函数从INSERT…UPDATE语句的INSERT部分引用列值。换句话说，如果没有发生重复关键字冲突，则UPDATE子句中的VALUES(col_name)可以引用被插入的col_name的值。本函数特别适用于多行插入。VALUES()函数只在INSERT…UPDATE语句中有意义，其它时候会返回NULL。12INSERT INTO table (a,b,c) VALUES (1,2,3),(4,5,6) ON DUPLICATE KEY UPDATE c=VALUES(a)+VALUES(b); 本语句与以下两个语句作用相同：12INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=3;INSERT INTO table (a,b,c) VALUES (4,5,6) ON DUPLICATE KEY UPDATE c=9; 当您使用ON DUPLICATE KEY UPDATE时，DELAYED选项被忽略。注：来源","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"特定国家版windows8","slug":"hack/特定国家版windows8","date":"2016-03-09T06:01:37.000Z","updated":"2017-11-04T08:30:45.782Z","comments":true,"path":"2016/03/09/hack/特定国家版windows8/","link":"","permalink":"http://yoursite.com/2016/03/09/hack/特定国家版windows8/","excerpt":"","text":"有强迫症的同学真是伤不起，他们就认定只有OEM厂商预装的系统才是正版的，其他通过密钥激活、kms激活的系统就是盗版的，为了满足这些同学的强迫症，笔者搜罗了半天网络资源，终于将OEM厂商使用的预装系统镜像收集完毕，他们分别是特定国家版（CoreCountrySpecific）和单语言版（CoreSingleLanguage），在大多华硕笔记本、联想笔记本中预装的系统通常显示为win8/8.1中文版，其实这些版本的实质就是特定国家版，今天给大家讲解一下他们与其他系统版本的区别以及分享一下他们的下载地址和有效安装密钥。 如何查看你预装的系统版本具体是什么？ win8/8.1中文版，可以通过注册表查看系统的具体版本，打开注册表定位到HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion，然后在右边找到EditionID和ProductName项，如果EditionID处显示为CoreCountrySpecific，ProductName处显示为Windows 8/8.1 China，那么就说明你预装的系统为特定国家版，如果EditionID处显示为CoreSingleLanguage，那么说明你预装的系统为单语言版，目前国内OEM厂商使用的基本就是这两种系统了。 特定国家版、单语言版与其他版本的区别是什么？ 一般预装系统显示为win8/8.1中文版的大多都是特定国家版（CoreCountrySpecific），细心的同学可能会发现不管特定国家版还是单语言版，他们的英文名字都带一个“Core”，Core是什么呢？大家都知道win8/8.1分为核心板、专业版、企业版，那么Core就是核心板，这说明特定国家版和单语言版的实质就是核心板，他们与核心版的不同之处：特定国家使用，并且无法更换系统语言，比如是中文版，无法通过语言包或语言设置更换成其他版本，另外他们的密钥也不通用。（本文由 亦是美网络 yishimei.cn 原创） 为什么有些同学哭死哭活的要安装win8/8.1预装系统版本？ OEM厂商在电脑出厂时就将密钥集成在主板里了，如果使用预装系统的版本，在安装成功后，输入OEM集成在主板里的密钥就可以直接联网激活了，那些同学心目中的“正版概念”就是这么来的。 win8.1 CoreCountrySpecific（特定国家版）下载地址： 32位系统：http://pan.baidu.com/s/1dDCNHdR 64位系统：http://pan.baidu.com/s/1c05ym0w 安装密钥：TNH8J-KG84C-TRMG4-FFD7J-VH4WX 激活方法： 1、使用安装密钥安装完成后，参考（oem预装系统主板密钥提取神器 - RW - Read &amp; Write utility ）获取系统集成的密钥，然后在线联网激活。 2、下载kms神龙版激活（KMS在线激活windows8/8.1、windows7和office2013/2010之MicroKMS 神龙版）。 win8.1 CoreSingleLanguage（单语言版）下载地址： 32位系统：http://pan.baidu.com/s/1bn3ytzx 64位系统：http://pan.baidu.com/s/1hqGLrRA 安装密钥：Y9NXP-XT8MV-PT9TG-97CT3-9D6TC 激活方法：同win8.1 CoreCountrySpecific（特定国家版）。","categories":[{"name":"windows","slug":"windows","permalink":"http://yoursite.com/categories/windows/"}],"tags":[{"name":"windows8","slug":"windows8","permalink":"http://yoursite.com/tags/windows8/"}]},{"title":"Spark基础知识","slug":"spark/Spark基础知识","date":"2016-03-09T05:59:50.000Z","updated":"2017-11-04T08:30:45.787Z","comments":true,"path":"2016/03/09/spark/Spark基础知识/","link":"","permalink":"http://yoursite.com/2016/03/09/spark/Spark基础知识/","excerpt":"","text":"Spark基本概念 RDD——Resillient Distributed Dataset A Fault-Tolerant Abstraction for In-Memory Cluster Computing弹性分布式数据集。 Operation——作用于RDD的各种操作分为transformation和action。 Job——作业，一个JOB包含多个RDD及作用于相应RDD上的各种operation。 Stage——一个作业分为多个阶段。 Partition——数据分区， 一个RDD中的数据可以分成多个不同的区。 DAG——Directed Acycle graph，有向无环图，反应RDD之间的依赖关系。 Narrow dependency——窄依赖，子RDD依赖于父RDD中固定的data partition。 Wide Dependency——宽依赖，子RDD对父RDD中的所有data partition都有依赖。 Caching Managenment——缓存管理，对RDD的中间计算结果进行缓存管理以加快整 体的处理速度。","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"运行第一个SparkStreaming程序（及过程中问题解决）","slug":"spark/SparkStreaming程序（及过程中问题解决）","date":"2016-03-09T04:51:43.000Z","updated":"2017-11-04T08:30:45.787Z","comments":true,"path":"2016/03/09/spark/SparkStreaming程序（及过程中问题解决）/","link":"","permalink":"http://yoursite.com/2016/03/09/spark/SparkStreaming程序（及过程中问题解决）/","excerpt":"","text":"官方示例说明按照官方文档的 这个示例说明，可以轻松的在本地的spark-shell环境中测试这个示例。示例，即为了更好的入门，那么就再说明一下。运行这个统计单词的方式有三种，前面两种是官方文档上的指引，第三种则是用scala程序运行。 第一种方式, run-demo 打开一个终端，打开一个终端，输入 命令 nc -lk 9999，暂时叫做 “nc终端” 吧 再打开终端，切换到Spark HOME目录， 执行命令 bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999， 然后每秒会有类似一下日志循环输出&gt;-——————————————Time: 1415701382000 ms-——————————————-——————————————Time: 1415701383000 ms-—————————————— 在nc终端随便输入一些字符串，用空格隔开，回车，如aa aa bb c。可以在上面的Spark终端中看到有新内容输出&gt;-——————————————Time: 1415701670000 ms-——————————————(aa,2)(bb,1)(c,1) OK，成功！ 第二种 spark-shell 模式下面介绍在spark-shell中输入scala代码运行的方式。 同上面第一步，打开一个终端，打开一个终端，输入 命令 nc -lk 9999，暂时叫做 “nc终端” 吧 再打开一个终端， 切换到Spark HOME目录下，输入 bin/spark-shell （如果你已经安装好了Spark的话，直接输入 spark-shell 即可），等待Spark启动成功，会打印信息&gt;Spark context available as sc.scala&gt; 然后输入以下语句： 123456789101112131415161718192021222324import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._import org.apache.spark.api.java.function._import org.apache.spark.streaming._import org.apache.spark.streaming.api._// Create a StreamingContext with a local masterval ssc = new StreamingContext(sc, Seconds(1))// Create a DStream that will connect to serverIP:serverPort, like localhost:9999val lines = ssc.socketTextStream(\"localhost\", 9999)// Split each line into wordsval words = lines.flatMap(_.split(\" \"))import org.apache.spark.streaming.StreamingContext._// Count each word in each batchval pairs = words.map(word =&gt; (word, 1))val wordCounts = pairs.reduceByKey(_ + _)// Print a few of the counts to the consolewordCounts.print()ssc.start() // Start the computationssc.awaitTermination() // Wait for the computation to terminate 会打印以下信息：&gt;14/11/11 18:07:23 INFO MemoryStore: ensureFreeSpace(2216) called with curMem=100936, maxMem=278019440.…..14/11/11 18:07:23 INFO DAGScheduler: Stage 91 (take at DStream.scala:608) finished in 0.004 s14/11/11 18:07:23 INFO SparkContext: Job finished: take at DStream.scala:608, took 0.007531701 s -——————————————Time: 1415700443000 ms-—————————————— 同第一种方式的第3步，随便输入一些字符串，用空格隔开，回车，如aa aa bb c。可以在上面的Spark终端中看到有新内容输出&gt;-——————————————Time: 1415701670000 ms-——————————————(aa,2)(bb,1)(c,1) OK，成功！ 第三种 scala-ide编程方式在用这种方式运行这个demo代码的时候，遇到了不少问题，记录下来，供大家参考。这个例子，请大家先根据这里记录的方式进行操作，得到一个可以运行的程序，后面我会记录遇到的问题。 下载scala-ide, 下载链接，下载 For Scala 2.10.4 下的对应平台的ide，解压，运行。 安装sbt，下载链接, 安装sbteclipse, github地址, 编辑 ~/.sbt/0.13/plugins/plugins.sbt 文件， 添加以下内容 addSbtPlugin(&quot;com.typesafe.sbteclipse&quot; % &quot;sbteclipse-plugin&quot; % &quot;2.5.0&quot;)，如果没有plugins目录和plugins.sbt，自行创建。 用向导创建一个scala项目，并在项目根目录下创建一个build.sbt文件，添加以下内容(注意，每行正式语句之后要换行) 1234567891011121314name := &quot;spark-test&quot;version := &quot;1.0&quot;scalaVersion := &quot;2.10.4&quot;// set the main class for the main &apos;run&apos; task// change Compile to Test to set it for &apos;test:run&apos;mainClass in (Compile, run) := Some(&quot;test.SparkTest&quot;)libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-streaming_2.10&quot; % &quot;1.1.0&quot; 创建test.SparkTest.scala文件，添加以下代码 1234567891011121314151617181920212223242526package testimport org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._import org.apache.spark.SparkContextimport org.apache.spark.api.java.function._import org.apache.spark.streaming._import org.apache.spark.streaming.api._object SparkTest &#123; def main(args: Array[String]): Unit = &#123; // Create a StreamingContext with a local master // Spark Streaming needs at least two working thread val ssc = new StreamingContext(\"local[2]\", \"NetworkWordCount\", Seconds(10)) // Create a DStream that will connect to serverIP:serverPort, like localhost:9999 val lines = ssc.socketTextStream(\"localhost\", 9999) // Split each line into words val words = lines.flatMap(_.split(\" \")) // Count each word in each batch val pairs = words.map(word =&gt; (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) wordCounts.print ssc.start ssc.awaitTermination &#125;&#125; 终端中切换目录到这个项目根目录，输入命令 sbt ， 命令运行成功后，敲入 eclipse 生成eclipse项目和项目所需依赖 同第一种方式的第1,3步，再打开一个终端，输入 命令 nc -lk 9999。然后运行刚才写的main程序，在nc终端中输入一些字符串，用空格隔开，回车，如aa aa bb c。可以在ide控制台中观察到&gt;-——————————————Time: 1415701670000 ms-——————————————(aa,2)(bb,1)(c,1) OK，成功！ 下面是遇到的问题及解决方法：1. 运行程序说找不到主类解：没有在sbt文件配置主类是哪个，在build.sbt 文件中添加以下代码 mainClass in (Compile, run) := Some(“test.SparkTest”) Some中就是主类的路径 2. java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce$class这个问题困扰了我很长时间，一直没找到怎么解决。后来看到说是scala每次版本升级不兼容以前的版本编译的库，于是换了对应的版本的ide才正常运行。解：scala-ide版本和现在用的spark包依赖编译的scala版本不一致， 请下载上面说过的 scala-ide For Scala 2.10.4 版本。","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"},{"name":"spark-streaming","slug":"spark-streaming","permalink":"http://yoursite.com/tags/spark-streaming/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming+Zookeeper+Kafka入门程序","slug":"spark/SparkStreaming+Zookeeper+Kafka入门程序","date":"2016-03-09T04:51:43.000Z","updated":"2017-11-04T08:30:45.787Z","comments":true,"path":"2016/03/09/spark/SparkStreaming+Zookeeper+Kafka入门程序/","link":"","permalink":"http://yoursite.com/2016/03/09/spark/SparkStreaming+Zookeeper+Kafka入门程序/","excerpt":"","text":"准备工作： 安装 spark 安装 zookeeper 安装 kafka 开始工作1. 启动zookeeper 打开终端，切换到 zookeeper HOME 目录， 进入conf文件夹，拷贝一份 zoo_sample.cfg 副本并重命名为 zoo.cfg 切换到上级的bin目录中，执行 ./zkServer.sh start 启动zookeeper，会有日志打印 Starting zookeeper … STARTED 然后用 ./zkServer.sh status 查看状态，如果有下列信息输出，则说明启动成功 Mode: standalone 如果要停止zookeeper，则运行 ./zkServer stop 即可 2. 启动kafka打开终端，切换到 kafka HOME 目录,运行 bin/kafka-server-start.sh config/server.properties 会有以下类似日志输出 &gt;[2014-11-12 17:38:13,395] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions [test,0] (kafka.server.ReplicaFetcherManager)[2014-11-12 17:38:13,420] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions [test,0] (kafka.server.ReplicaFetcherManager) 3. 启动kafka生产者重新打开一个终端，暂叫做 生产者终端，方便后面引用说明。切换到 kafka HOME 目录,运行 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 创建一个叫 test 的主题。 4. 编写scala应用程序123456789101112131415161718192021222324252627282930313233343536373839404142434445 package test import java.util.Properties import kafka.producer._ import org.apache.spark.streaming._ import org.apache.spark.streaming.StreamingContext._ import org.apache.spark.streaming.kafka._ import org.apache.spark.SparkConf object KafkaWordCount &#123; def main(args: Array[String]) &#123; // if (args.length &lt; 4) &#123; // System.err.println(\"Usage: KafkaWordCount &lt;zkQuorum&gt; &lt;group&gt; &lt;topics&gt; &lt;numThreads&gt;\") // System.exit(1) // &#125; // StreamingExamples.setStreamingLogLevels() //val Array(zkQuorum, group, topics, numThreads) = args val zkQuorum = \"localhost:2181\" val group = \"1\" val topics = \"test\" val numThreads = 2 val sparkConf = new SparkConf().setAppName(\"KafkaWordCount\").setMaster(\"local[2]\") val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.checkpoint(\"checkpoint\") val topicpMap = topics.split(\",\").map((_,numThreads)).toMap val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicpMap).map(_._2) val words = lines.flatMap(_.split(\" \")) val pairs = words.map(word =&gt; (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) //val wordCounts = words.map(x =&gt; (x, 1L)) // .reduceByKeyAndWindow(_ + _, _ - _, Minutes(10), Seconds(2), 2) wordCounts.print() ssc.start() ssc.awaitTermination() &#125;&#125; build.sbt 文件中添加依赖 &gt; libraryDependencies += “org.apache.spark” % “spark-streaming_2.10” % “1.1.0”&gt;libraryDependencies += “org.apache.spark” % “spark-streaming-kafka_2.10” % “1.1.0” 启动scala程序，然后在 上面第2步的 生产者终端中输入一些字符串，如 sdfsadf a aa a a a a a a a a ，在ide的控制台上可以看到有信息输出 &gt;4/11/12 16:38:22 INFO scheduler.DAGScheduler: Stage 195 (take at DStream.scala:608) finished in 0.004 s-——————————————Time: 1415781502000 ms-——————————————(aa,1)(a,9)(sdfsadf,1) 说明程序成功运行。","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"},{"name":"spark-streaming","slug":"spark-streaming","permalink":"http://yoursite.com/tags/spark-streaming/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Apache-Sqoop 安装","slug":"sqoop/Apache-Sqoop-安装","date":"2016-03-09T04:51:43.000Z","updated":"2017-11-04T08:30:45.792Z","comments":true,"path":"2016/03/09/sqoop/Apache-Sqoop-安装/","link":"","permalink":"http://yoursite.com/2016/03/09/sqoop/Apache-Sqoop-安装/","excerpt":"","text":"准备首先当然是下载sqoopsqoop 依赖以下软件,点击链接可以直接下载&gt;hadoopaccumuloapache-hivehbasezookeeper 配置配置JAVA环境变量JAVA_HOME=/home/du/software/dev/jdk1.7.0_45123export JAVA_HOME=/usr/install/java #此处换成自己的jdk目录export CLASSPATH=.:$JAVA_HOME/jre/libexport PATH=$PATH:$JAVA_HOME/bin 配置sqoop运行依赖12345678export HADOOP_COMMON_HOME=/home/du/software/dev/hadoop-2.6.0export HADOOP_MAPRED_HOME=$HADOOP_COMMON_HOME/share/hadoop/mapreduceexport ZOOKEEPER_HOME=/home/du/software/dev/zookeeper-3.4.6export ACCUMULO_HOME=/usr/install/accumulo-1.6.2export HIVE_HOME=/usr/install/apache-hive-1.0.0-binexport HCAT_HOME=/usr/install/apache-hive-1.0.0-bin/hcatalogexport HBASE_HOME=/usr/install/hbase-1.0.0export SQOOP_HOME=/usr/install/sqoop-1.4.4.bin__hadoop-2.0.4-alpha 测试切换到sqoop目录，运行 bin/sqoop help， 如果打印帮助文档则说明成功。","categories":[{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/categories/sqoop/"}],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/tags/sqoop/"}]},{"title":"Sqoop使用","slug":"sqoop/Sqoop使用","date":"2016-03-09T04:51:43.000Z","updated":"2017-11-04T08:30:45.792Z","comments":true,"path":"2016/03/09/sqoop/Sqoop使用/","link":"","permalink":"http://yoursite.com/2016/03/09/sqoop/Sqoop使用/","excerpt":"","text":"sqoop help 查看帮助信息sqoop help COMMAND 查看 COMMAND具体的帮助，如要查看 list-databases 命令的用法，则使用 sqoop help list-databases 查看。 主要可用的命令如下： 命令 功能 help List available commands import Import a table from a database to HDFS list-databases List available databases on a server list-tables List available tables in a database 主要参数说明 参数 说明 –connect 用来指定jdbc链接url，如mysql的: jdbc:mysql://ip:port/database –password 指定密码， 安全起见，建议使用 -P 参数，交互式填写密码或者使用 –password-file参数 –password-file 指定密码的文件，从该文件中读取密码 –username 指定用户名 用help查看帮助，使用示例：list-databases 是列出所有的数据库，sqoop help list-databases· 查看使用方法 使用示例，查看 本机上的mysql中的数据库./sqoop list-databases –connect jdbc:mysql://127.0.0.1:3306/test –username username -P这样直接操作会提示找不到驱动，我们需要把对应的mysql驱动jar包放到$SQOOP/lib目录下，然后再次执行就可以了，或者用参数 -libjars 指定驱动jar包路径。 配置项说明按照此处的配置项进行可避免文末的错误，如果遇到错误请参考文末错误说明和解决方法。 sqoop 要使用对应的hadoop版本，如使用的hadoo版本是2.0.4，那么对应的sqoop版本就要使用文件名包含hadoop2.0.4的信息的版本。 SQOOP_HOME 环境变量关系到sqoop运行时选择的版本问题，所以该变量请配置成正确的版本路径。如果配置成了别的，虽然执行命令是在正确的路径下执行，而真实运行的版本却是其他的版本，该问题可以通过运行sqoop version 查看，此问题比较隐晦，要注意。 执行sqoop所对应的SQOOP_HOME 文件要和hdfs文件系统上的一致，否则会产生找不到对应库文件的错误。 在/etc/hosts 文件中增加 archeagle 到 hdfs节点ip的映射，否则sqoop会用默认的ip映射，会连接不上。 用户权限问题，可以在 文件 hadoop/etc/hadoop/hdfs-site.xml中增加或者修改 配置 12345678 &lt;property&gt; &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; hdfs 集群要启动yarn服务。 import 的使用常用参数说明 参数 说明 -fs 指定hdfs节点 –target-dir 要到处到hdfs文件系统上的文件路径 –table 要导出的表名 –connect jdbc url –username 数据库用户名 -P 从控制台输入密码 使用示例 ：1bin/sqoop import -fs hdfs://192.168.6.63:9000 --target-dir /user/admin/export_test_admin_user11 --table admin_user --connect jdbc:mysql://192.168.6.201:3306/test --username username -P 增量导入 原始链接主要参数如下： 参数 说明 –incrementa 增量方式， 有两种方式，lastmodified和append –last-value 以lastmodified方式的增量追加，要指定时间；append则要指定偏移id –check-column 要检查的字段， 即以哪个字段为标准计算增量范围 –append 指定以增量方式追加 使用增量导入（以时间为标识作参考）：1bin/sqoop import -fs hdfs://192.168.6.63:9000 --target-dir /user/admin/export_test_admin_user11 --table admin_user --connect jdbc:mysql://192.168.6.201:3306/forseti_core --username forseti -P--incremental lastmodified --check-column gmt_create --last-value '2012-02-01 11:0:00' --verbose --append 使用增量导入（以id为标识作为参考）：1bin/sqoop import -fs hdfs://192.168.6.63:9000 --target-dir /user/admin/export_test_admin_user11 --table admin_user --connect jdbc:mysql://192.168.6.201:3306/forseti_core --username forseti -P--incremental append --check-column id --verbose --append 使用select语句(-e或者–query参数)如果使用这个参数，那么可以执行自定义语句，比如可以执行join操作等其他复杂sql语句，但是语句中where是必须的，而且where后面要加 $CONDITIONS 参数。sql语句本身可以用单引号包裹，但是如果sql语句中已经包含了单引号，那么可以用双引号包裹。另外，使用了这个参数，那么参数 –split-by 在import命令中是必须的，而且该参数后面指定的字段必须出现在sql查询结果中。因为通过观察sqoop执行过程中输出的执行sql可以发现，它是在原有的sql上包裹一层，如下示例中，结果就变成了 SELECT MIN(gmt_modified), MAX(gmt_modified) FROM (select id from admin_user where (1 = 1) ) AS t1。使用示例：1bin/sqoop import --connect jdbc:mysql://192.168.6.201:3306/test --username username -P -e \"select id from test where $CONDITIONS\" --split-by id job 使用主要参数 参数 说明 –create Create a new saved job –delete Delete a saved job –exec Run a saved job –help Print usage instructions –list List saved jobs –show Show the parameters for a saved job -fs &lt;local namenode:port&gt; specify a namenode -libjars specify comma separated jar files to include in the classpath. -conf specify an application configuration file 创建Job示例：1bin/sqoop job --create export_mysql_table -- import --table admin_user --connect jdbc:mysql://192.168.6.201:3306/forseti_core 执行Job示例：1bin/sqoop job -fs hdfs://192.168.6.63:9000 --exec export_mysql_table -- --username forseti -P --target-dir /user/admin/export_test_admin_user11112 执行带密码的任务有密码要求的任务，如果不存储密码的话，每次执行任务都要求手动输入密码，如果是定时任务，那么这个肯定是不合理的。默认metastore是不保存密码的，如果需要保存，则在conf/sqoop-site.xml增加或者取消注释如下内容 123456&lt;property&gt; &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, allow saved passwords in the metastore. &lt;/description&gt; &lt;/property&gt; 错误解决 ERROR tool.ImportTool: Encountered IOException running import job: java.io.FileNotFoundException: File does not exist: hdfs://192.168.6.63:9000/home/du/software/dev/sqoop-1.4.5.bin__hadoop-0.20/lib/ant-contrib-1.0b3.jar 在不同机器或者用户下执行sqoop，会查找hadoop集群指定的节点上的hdfs目录中的这个文件，比如我是用在/home/du/software/dev/sqoop-1.4.5.bin__hadoop-0.20 下执行的sqoop，并且SQOOP_HOME配置的也是这个路径，那么到hdfs://192.168.6.63:9000上就会查找/home/du/software/dev/sqoop-1.4.5.bin__hadoop-0.20/lib这个路径下的ant-contrib-1.0b3.jar这个文件，解决方法就是在hdfs上创建对应目录，并把sqoop拷贝到对应目录，目录结构和执行sqoop的目录结构一样即可。 Exception in thread “main” java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected 使用的hadoop版本问题，从2.6.0切换到2.4.0 解决 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.security.AccessControlException: Permission denied: user=du, access=WRITE, inode=”/user”:admin:supergroup:drwxr-xr-x ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Access denied for user ‘forseti’@’192.168.6.165’ (using password: YES) 很明显是mysql的用户登陆失败，填写正确的用户名和密码即可解决该问题。 15/03/05 17:40:10 INFO mapreduce.Job: Running job: job_1425543105230_000615/03/05 17:40:44 INFO ipc.Client: Retrying connect to server: archeagle/220.250.64.20:43175. Already tried 0 time(s); maxRetries=315/03/05 17:41:04 INFO ipc.Client: Retrying connect to server: archeagle/220.250.64.20:43175. Already tried 1 time(s); maxRetries=315/03/05 17:41:24 INFO ipc.Client: Retrying connect to server: archeagle/220.250.64.20:43175. Already tried 2 time(s); maxRetries=315/03/05 17:41:44 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=FAILED. Redirecting to job history server15/03/05 17:41:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Job status not available at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:322) at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599) at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1344) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1306) at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:186) at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:159) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:247) at org.apache.sqoop.manager.DirectMySQLManager.importTable(DirectMySQLManager.java:92) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) 在运行sqoop的主机hosts文件增减加hadoop节点ip映射 192.168.6.63 archeagle 使用–direct参数Error: java.io.IOException: Cannot run program “mysqldump”: error=2, No such file or directory at java.lang.ProcessBuilder.start(ProcessBuilder.java:1047) at java.lang.Runtime.exec(Runtime.java:617) at java.lang.Runtime.exec(Runtime.java:485) at org.apache.sqoop.mapreduce.MySQLDumpMapper.map(MySQLDumpMapper.java:405) at org.apache.sqoop.mapreduce.MySQLDumpMapper.map(MySQLDumpMapper.java:49) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)Caused by: java.io.IOException: error=2, No such file or directory at java.lang.UNIXProcess.forkAndExec(Native Method) at java.lang.UNIXProcess.(UNIXProcess.java:186) at java.lang.ProcessImpl.start(ProcessImpl.java:130) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028) … 12 more ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RetriableException): org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/du/.staging/job_1425543105230_0010. Name node is in safe mode.The reported blocks 0 needs additional 963 blocks to reach the threshold 0.9990 of total blocks 963.The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1199)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:3336)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInt(FSNamesystem.java:3296)at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3280)at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:727) hdfs上(用户)目录不存在。 INFO ipc.Client: Retrying connect to server: arch57/220.250.64.20:56564. Already tried 2 time(s); maxRetries=315/03/10 15:47:55 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server15/03/10 15:47:55 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Job status not available at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:322) at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599) at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1344) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1306) at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:186) at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:159) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:247) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:665) at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:118) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601) at org.apache.sqoop.tool.JobTool.execJob(JobTool.java:228) at org.apache.sqoop.tool.JobTool.run(JobTool.java:283) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) 在执行sqoop的机器的hosts增加 arch57 这个主机ip映射（PS:arch57 是一台hadoop机器的名字）","categories":[{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/categories/sqoop/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://yoursite.com/tags/hdfs/"},{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"},{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/tags/sqoop/"}]}]}